<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql表设计优化]]></title>
    <url>%2F2018%2Fmysql-table.html</url>
    <content type="text"><![CDATA[表的3NF (范式)表的范式，是首先符合1NF, 才能满足2NF , 进一步满足3NF。1NF: 即表的列的具有原子性,不可再分解，即列的信息，不能分解, 只有数据库是关系型数据库(mysql/oracle/db2/informix/sysbase/sql server)，就自动的满足1NF。2NF: 表中的记录是唯一的, 就满足2NF, 通常我们设计一个主键来实现，主键一般不含业务逻辑。3NF: 即表中不要有冗余数据, 就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放。所谓的范式，是用来学习参考的，设计的时候根据情况，未必一定要遵守。因为在数据库数据量特别大，并且访问并发也大的情况下，可能要采用反范式设计来提高数据库响应速度。mysql表的存储引擎选择myisam 存储: 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. ,比如 bbs 中的 发帖表，回复表.INNODB 存储: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表.Memory 存储: 比如我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快.一般情况可以选择MyISAM存储引擎，如果需要事务支持必须使用InnoDB存储引擎。业务开发中大多数情况建议使用InnoDB存储引擎。注意：MyISAM存储引擎 B-tree索引有一个很大的限制：参与一个索引的所有字段的长度之和不能超过1000字节。另外MyISAM数据和索引是分开，而InnoDB的数据存储是按聚簇(cluster)索引有序排列的，主键是默认的聚簇(cluster)索引，因此MyISAM虽然在一般情况下，查询性能比InnoDB高，但InnoDB的以主键为条件的查询性能是非常高的。如果你的数据库的存储引擎是myisam,请一定记住要定时进行碎片整理。建表规约（主要参考阿里巴巴java开发手册）【强制】表达是与否概念的字段，必须使用 is_xxx 的方式命名，数据类型是 unsigned tinyint ( 1表示是，0表示否)。说明:任何字段如果为非负数，必须是 unsigned。正例:表达逻辑删除的字段名 is_deleted，1 表示删除，0 表示未删除。【强制】表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只 出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。 说明:MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写。因此，数据库 名、表名、字段名，都不允许出现任何大写字母，避免节外生枝。 正例:aliyun_admin，rdc_config，level3_name 反例:AliyunAdmin，rdcConfig，level_3_name【强制】表名不使用复数名词。 说明:表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数 形式，符合表达习惯。【强制】禁用保留字，如 desc、range、match、delayed 等，请参考 MySQL 官方保留字。【强制】主键索引名为 pk字段名;唯一索引名为 uk字段名;普通索引名则为 idx字段名。说明:pk 即 primary key;uk 即 unique key;idx 即 index 的简称。【强制】小数类型为 decimal，禁止使用 float 和 double。说明:float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不 正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。建议使用UNSIGNED存储非负数值，建议使用INT UNSIGNED存储IPV4，整形定义中不添加长度，比如使用INT，而不是INT(4)，使用短数据类型，比如取值范围为0-80时，使用TINYINT UNSIGNED，不建议使用ENUM类型，使用TINYINT来代替。【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。【强制】varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长 度大于此值，定义字段类型为 text，独立出来一张表，用主键来对应，避免影响其它字段索 引效率。VARCHAR(N)，N表示的是字符数不是字节数，比如VARCHAR(255)，可以最大可存储255个汉字，需要根据实际的宽度来选择N。VARCHAR(N)，N尽可能小，因为MySQL一个表中所有的VARCHAR字段最大长度是65535个字节，进行排序和创建临时表一类的内存操作时，会使用N的长度申请内存。【强制】表必备三字段:id, gmt_create, gmt_modified。 说明:其中id必为主键，类型为unsigned bigint、单表时自增、步长为1。gmt_create, gmt_modified 的类型均为 TIMESTAMP 类型，前者现在时表示主动创建，后者过去分词表示被 动更新。【推荐】表的命名最好是加上“业务名称_表的作用”。 正例:alipay_task / force_project / trade_config【推荐】库名与应用名称尽量一致。【推荐】如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。【推荐】字段允许适当冗余，以提高查询性能，但必须考虑数据一致。冗余字段应遵循: 1)不是频繁修改的字段。2)不是 varchar 超长字段，更不能是 text 字段。正例:商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存 储类目名称，避免关联查询。【推荐】单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。 说明:如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检 索速度。【推荐】表字符集选择UTF8，将过大字段拆分到其他表中，禁止在数据库中使用VARBINARY、BLOB存储图片、文件等。时间字段尽量使用TIMESTAMP类型，因为其存储空间只需要 DATETIME 类型的一半，且日期类型中只有它能够和实际时区相对应。对于只需要精确到某一天的数据类型，建议使用DATE类型，因为他的存储空间只需要3个字节，比TIMESTAMP还少。数据库索引1、：业务需要的相关索引是根据实际的设计所构造sql语句的where条件来确定的，业务不需要的不要建索引，不允许在联合索引（或主键）中存在多于的字段。特别是该字段根本不会在条件语句中出现。2、：唯一确定一条记录的一个字段或多个字段要建立主键或者唯一索引，不能唯一确定一条记录，为了提高查询效率建普通索引3、：业务使用的表，有些记录数很少，甚至只有一条记录，为了约束的需要，也要建立索引或者设置主键。4、：对于取值不能重复，经常作为查询条件的字段，应该建唯一索引(主键默认唯一索引)，并且将查询条件中该字段的条件置于第一个位置。没有必要再建立与该字段有关的联合索引。5、：对于经常查询的字段，其值不唯一，也应该考虑建立普通索引，查询语句中该字段条件置于第一个位置，对联合索引处理的方法同样。6、：业务通过不唯一索引访问数据时，需要考虑通过该索引值返回的记录稠密度，原则上可能的稠密度最大不能高于0.2，如果稠密度太大，则不合适建立索引了。当通过这个索引查找得到的数据量占到表内所有数据的20%以上时，则需要考虑建立该索引的代价，同时由于索引扫描产生的都是随机I/O，生其效率比全表顺序扫描的顺序I/O低很多。数据库系统优化query的时候有可能不会用到这个索引。7、：需要联合索引(或联合主键)的数据库要注意索引的顺序。SQL语句中的匹配条件也要跟索引的顺序保持一致。注意：索引的顺势不正确也可能导致严重的后果。8、：表中的多个字段查询作为查询条件，不含有其他索引，并且字段联合值不重复，可以在这多个字段上建唯一的联合索引，假设索引字段为 (a1,a2,…an),则查询条件(a1 op val1,a2 op val2,…am op valm)m&lt;=n,可以用到索引，查询条件中字段的位置与索引中的字段位置是一致的。9、：联合索引的建立原则(以下均假设在数据库表的字段a,b,c上建立联合索引(a,b,c))联合索引中的字段应尽量满足过滤数据从多到少的顺序，也就是说差异最大的字段应该房子第一个字段建立索引尽量与SQL语句的条件顺序一致，使SQL语句尽量以整个索引为条件，尽量避免以索引的一部分(特别是首个条件与索引的首个字段不一致时)作为查询的条件注意联合索引顺序当需要查询的数据库字段全部在索引中体现时，数据库可以直接查询索引得到查询信息无须对整个表进行扫描(这就是所谓的key-only)，能大大的提高查询效率。当a，ab，abc与其他表字段关联查询时可以用到索引当a，ab，abc顺序而不是b，c，bc，ac为顺序执行Order by或者group不要时可以用到索引以下情况时，进行表扫描然后排序可能比使用联合索引更加有效a.表已经按照索引组织好了b.被查询的数据站所有数据的很多比例。10、：重要业务访问数据表时。但不能通过索引访问数据时，应该确保顺序访问的记录数目是有限的，原则上不得多于10.更多索引知识参考mysql高性能索引业务主键和逻辑主键业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。复合主键（联合主键）：通过两个或者多个字段的组合作为主键。原理分析：使用逻辑主键的主要原因是，业务主键一旦改变则系统中关联该主键的部分的修改将会是不可避免的，并且引用越多改动越大。而使用逻辑主键则只需要修改相应的业务主键相关的业务逻辑即可，减少了因为业务主键相关改变对系统的影响范围。业务逻辑的改变是不可避免的，因为“永远不变的是变化”，没有任何一个公司是一成不变的，没有任何一个业务是永远不变的。最典型的例子就是身份证升位和驾驶执照号换用身份证号的业务变更。而且现实中也确实出现了身份证号码重复的情况，这样如果用身份证号码作为主键也带来了难以处理的情况。当然应对改变，可以有很多解决方案，方案之一是做一新系统与时俱进，这对软件公司来说确实是件好事。使用逻辑主键的另外一个原因是，业务主键过大，不利于传输、处理和存储。我认为一般如果业务主键超过8字节就应该考虑使用逻辑主键了，因为int是4字节的，bigint是8字节的，而业务主键一般是字符串，同样是 8 字节的 bigint 和 8 字节的字符串在传输和处理上自然是 bigint 效率更高一些。想象一下 code == “12345678” 和 id == 12345678 的汇编码的不同就知道了。当然逻辑主键不一定是 int 或者 bigint ，而业务主键也不一定是字符串也可以是 int 或 datetime 等类型，同时传输的也不一定就是主键，这个就要具体分析了，但是原理类似，这里只是讨论通常情况。同时如果其他表需要引用该主键的话，也需要存储该主键，那么这个存储空间的开销也是不一样的。而且这些表的这个引用字段通常就是外键，或者通常也会建索引方便查找，这样也会造成存储空间的开销的不同，这也是需要具体分析的。使用逻辑主键的再一个原因是，使用 int 或者 bigint 作为外键进行联接查询，性能会比以字符串作为外键进行联接查询快。原理和上面的类似，这里不再重复。使用逻辑主键的再一个原因是，存在用户或维护人员误录入数据到业务主键中的问题。例如错把 RMB 录入为 RXB ，相关的引用都是引用了错误的数据，一旦需要修改则非常麻烦。如果使用逻辑主键则问题很好解决，如果使用业务主键则会影响到其他表的外键数据，当然也可以通过级联更新方式解决，但是不是所有都能级联得了的。使用业务主键的主要原因是，增加逻辑主键就是增加了一个业务无关的字段，而用户通常都是对于业务相关的字段进行查找（比如员工的工号，书本的 ISBN No. ），这样我们除了为逻辑主键加索引，还必须为这些业务字段加索引，这样数据库的性能就会下降，而且也增加了存储空间的开销。所以对于业务上确实不常改变的基础数据而言，使用业务主键不失是一个比较好的选择。另一方面，对于基础数据而言，一般的增、删、改都比较少，所以这部分的开销也不会太多，而如果这时候对于业务逻辑的改变有担忧的话，也是可以考虑使用逻辑主键的，这就需要具体问题具体分析了。使用业务主键的另外一个原因是，对于用户操作而言，都是通过业务字段进行的，所以在这些情况下，如果使用逻辑主键的话，必须要多做一次映射转换的动作。我认为这种担心是多余的，直接使用业务主键查询就能得到结果，根本不用管逻辑主键，除非业务主键本身就不唯一。另外，如果在设计的时候就考虑使用逻辑主键的话，编码的时候也是会以主键为主进行处理的，在系统内部传输、处理和存储都是相同的主键，不存在转换问题。除非现有系统是使用业务主键，要把现有系统改成使用逻辑主键，这种情况才会存在转换问题。暂时没有想到还有什么场景是存在这样的转换的。使用业务主键的再一个原因是，对于银行系统而言安全性比性能更加重要，这时候就会考虑使用业务主键，既可以作为主键也可以作为冗余数据，避免因为使用逻辑主键带来的关联丢失问题。如果由于某种原因导致主表和子表关联关系丢失的话，银行可是会面临无法挽回的损失的。为了杜绝这种情况的发生，业务主键需要在重要的表中有冗余存在，这种情况最好的处理方式就是直接使用业务主键了。例如身份证号、存折号、卡号等。所以通常银行系统都要求使用业务主键，这个需求并不是出于性能的考虑而是出于安全性的考虑。使用复合主键的主要原因和使用业务主键是相关的，通常业务主键只使用一个字段不能解决问题，那就只能使用多个字段了。例如使用姓名字段不够用了，再加个生日字段。这种使用复合主键方式效率非常低，主要原因和上面对于较大的业务主键的情况类似。另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。使用复合主键的另外一个原因是，对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。综合来说，网上大多数人是倾向于用逻辑主键的，而对于实体表用复合主键方式的应该没有多少人认同。支持业务主键的人通常有种误解，认为逻辑主键必须对用户来说有意义，其实逻辑主键只是系统内部使用的，对用户来说是无需知道的。结论或推论：1、尽量避免使用业务主键，尽量使用逻辑主键。2、如果要使用业务主键必须保证业务主键相关的业务逻辑改变的概率为0，并且业务主键不太大，并且业务主键不能交由用户修改。3、除关系表外，尽量不使用复合主键。表的拆分1、垂直拆分（其实就是列的拆分将原来的一个有很多列的表拆分成多张表）注意：垂直拆分应该在数据表设计之初就执行的步骤，然后查询的时候用jion关键起来即可;通常我们按以下原则进行垂直拆分:把不常用的字段单独放在一张表;把text，blob等大字段拆分出来放在附表中;经常组合查询的列放在一张表中;缺点也很明显，需要使用冗余字段，而且需要join操作。一句话: 如果一张表某个字段，信息量大，但是我们很少查询，则可以考虑把这些字段，单独的放入到一张表中，这种方式称为垂直分割.示意图:2、水平拆分（ 如果你发现某个表的记录太多，例如超过一千万条，则要对该表进行水平分割。水平分割的做法是，以该表主键的某个值为界线，将该表的记录水平分割为两个表。）当然，我们还可以用增量法。如流水这类不会改变的数据,我们用增量查询。1.创建一张日充值表,记录每天充值总额2.每天用定时器对当前充值记录进行结算3.创建每月充值表,每月最后一天用定时器计算总额4.则要查询总额,则从月报表中汇总，再从日报表查询当天之前的数据汇总，再加上今天的使用当天流水表记录今天的流水，三张表加起来,汇总。这样子效率是极好的！3、传说中的‘三少原则’①：数据库的表越少越好.②：表的字段越少越好.③：字段中的组合主键、组合索引越少越好.当然这里的少是相对的，是减少数据冗余的重要设计理念。TIMESTAMP和DATETIME1&gt; 两者的存储方式不一样对于TIMESTAMP，它把客户端插入的时间从当前时区转化为UTC（世界标准时间）进行存储。查询时，将其又转化为客户端当前时区进行返回。而对于DATETIME，不做任何改变，基本上是原样输入和输出。123456789101112131415161718192021222324252627mysql&gt; create table test(id int,hiredate timestamp);Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into test values(1,'20151208000000');Query OK, 1 row affected (0.00 sec)mysql&gt; create table test1(id int,hiredate datetime);Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into test1 values(1,'20151208000000');Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test;+------+---------------------+| id | hiredate |+------+---------------------+| 1 | 2015-12-08 00:00:00 |+------+---------------------+1 row in set (0.01 sec)mysql&gt; select * from test1;+------+---------------------+| id | hiredate |+------+---------------------+| 1 | 2015-12-08 00:00:00 |+------+---------------------+1 row in set (0.00 sec) 两者输出是一样的。 其次修改当前会话的时区123456789101112131415161718192021222324252627mysql&gt; show variables like '%time_zone%'; +------------------+--------+| Variable_name | Value |+------------------+--------+| system_time_zone | CST || time_zone | SYSTEM |+------------------+--------+2 rows in set (0.00 sec)mysql&gt; set time_zone='+0:00';Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test;+------+---------------------+| id | hiredate |+------+---------------------+| 1 | 2015-12-07 16:00:00 |+------+---------------------+1 row in set (0.00 sec)mysql&gt; select * from test1;+------+---------------------+| id | hiredate |+------+---------------------+| 1 | 2015-12-08 00:00:00 |+------+---------------------+1 row in set (0.01 sec) 上述“CST”指的是MySQL所在主机的系统时间，是中国标准时间的缩写，China Standard Time UT+8:00通过结果可以看出，test中返回的时间提前了8个小时，而test1中时间则不变。这充分验证了两者的区别。 2&gt; 两者所能存储的时间范围不一样timestamp所能存储的时间范围为：’1970-01-01 00:00:01.000000’ 到 ‘2038-01-19 03:14:07.999999’。datetime所能存储的时间范围为：’1000-01-01 00:00:00.000000’ 到 ‘9999-12-31 23:59:59.999999’。 总结：TIMESTAMP和DATETIME除了存储范围和存储方式不一样，没有太大区别。当然，对于跨时区的业务，TIMESTAMP更为合适。 关于TIMESTAMP和DATETIME的自动初始化和更新12345678910111213141516171819202122mysql&gt; create table test(id int,hiredate timestamp);Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into test(id) values(1);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test;+------+---------------------+| id | hiredate |+------+---------------------+| 1 | 2015-12-08 14:34:46 |+------+---------------------+1 row in set (0.00 sec)mysql&gt; show create table test\G*************************** 1. row *************************** Table: testCreate Table: CREATE TABLE `test` ( `id` int(11) DEFAULT NULL, `hiredate` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP) ENGINE=InnoDB DEFAULT CHARSET=latin11 row in set (0.00 sec) 看起来是不是有点奇怪，我并没有对hiredate字段进行插入操作，它的值自动修改为当前值，而且在创建表的时候，我也并没有定义“show create table test\G”结果中显示的“ DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP”。 其实，这个特性是自动初始化和自动更新（Automatic Initialization and Updating）。 自动初始化指的是如果对该字段（譬如上例中的hiredate字段）没有显性赋值，则自动设置为当前系统时间。 自动更新指的是如果修改了其它字段，则该字段的值将自动更新为当前系统时间。 它与“explicit_defaults_for_timestamp”参数有关。 默认情况下，该参数的值为OFF，如下所示：1234567mysql&gt; show variables like '%explicit_defaults_for_timestamp%';+---------------------------------+-------+| Variable_name | Value |+---------------------------------+-------+| explicit_defaults_for_timestamp | OFF |+---------------------------------+-------+1 row in set (0.00 sec) 下面我们看看官档的说明： By default, the first TIMESTAMP column has both DEFAULT CURRENT_TIMESTAMP and ON UPDATE CURRENT_TIMESTAMP if neither is specified explicitly。 很多时候，这并不是我们想要的，如何禁用呢？ 将“explicit_defaults_for_timestamp”的值设置为ON。 “explicit_defaults_for_timestamp”的值依旧是OFF，也有两种方法可以禁用 1&gt; 用DEFAULT子句该该列指定一个默认值 2&gt; 为该列指定NULL属性。 如下所示： 1234567891011121314151617181920212223mysql&gt; create table test1(id int,hiredate timestamp null);Query OK, 0 rows affected (0.01 sec)mysql&gt; show create table test1\G*************************** 1. row *************************** Table: test1Create Table: CREATE TABLE `test1` ( `id` int(11) DEFAULT NULL, `hiredate` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin11 row in set (0.00 sec)mysql&gt; create table test2(id int,hiredate timestamp default 0);Query OK, 0 rows affected (0.01 sec)mysql&gt; show create table test2\G*************************** 1. row *************************** Table: test2Create Table: CREATE TABLE `test2` ( `id` int(11) DEFAULT NULL, `hiredate` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00') ENGINE=InnoDB DEFAULT CHARSET=latin11 row in set (0.00 sec) 在MySQL 5.6.5版本之前，Automatic Initialization and Updating只适用于TIMESTAMP，而且一张表中，最多允许一个TIMESTAMP字段采用该特性。从MySQL 5.6.5开始，Automatic Initialization and Updating同时适用于TIMESTAMP和DATETIME，且不限制数量。参考（1）《高性能MySQL》（2）《阿里巴巴java开发手册》]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高智商IT精英们的出路是降级，而非升级]]></title>
    <url>%2F2018%2Farticle-1.html</url>
    <content type="text"><![CDATA[从前有三个屌丝，聚在一起做网络，提供免费的网络服务，砸锅卖铁，通宵达旦，除了卖肾啥都做了。3年后终于做到了五百万用户，对于年轻人来说，能把五百万人玩弄于鼓掌之间，已经是很牛逼轰轰的事了，不过用户越多，成本越高，每年服务器、带宽租金、房租水电、广告运营等成本，已经达到了十七八万，屌丝们不得不面对一个终极问题：如何盈利？屌丝们定了三盘沙县水饺，围着一箱子的冰啤酒开始计算：按照最近一月的登陆情况来看，四百万个账号已经不活跃了，真正有商业价值的只有一百万人，如 果开通xx功能，收点高级会员费，让其中1%的人升级为高级会员，每年付30块钱年费，那么每年收入就是100万x1%x30元=30万元！不错嘛， 扣除十七八万的运营成本，还剩毛利润12万，每个屌丝年底能分到4万大洋，如果按照打工者的算法，这三个人每人月薪3333元，木有奖金，木有津贴、木有任何福利，上班还得带自家的电脑。尽管如此，屌丝们还是激动得感谢苍天！我们终于要盈利啦！那一夜，人们看到三个发疯的屌丝在屋顶翩翩起舞。韩寒说，中国人民是最有忍耐力的族群，一点好处就感激涕零。他一定不知道，IT创业界里的屌丝，才是这群傻逼中的战斗机。他们可以平静地忍受每年都持续亏钱，而且还能信心十足的对所有人说公司的状态非常好，如果有一天居然收支平衡了，他们会激动的彻夜难眠，比北朝鲜倒掉还开心。这三个屌丝其实是非常幸运的，至少能做到月薪3333元。大部分的屌丝在第一年做到几万用户的时候就会挂掉，原因众多，最主要要的是意志太弱，受不了最初的寂寞；意志稍微坚强点的会在第二年第三年慢慢挂掉，原因主要是资金断裂、团队分裂；能成功熬到第四年还没饿死、还没被口水淹死、还没被肠胃病颈椎病腰肌劳损折磨死的，甚至员工不减反增的，基本上属于神仙级别了。我为什么要说这个小故事呢。首先是因为这是身边每天都在发生的故事，其次是因为感到可惜，IT界在我眼里一直是一个无比高级的职业，聚集着全球最聪明、最富有的人类精英。以IT创业界的青年们的智商，他们可以做成任何一件事情，包括改造银行到制造汽车到发射航天飞机 。结果这帮人却整天在蓬头垢面得为3k的月薪而挣扎，太悲催了。为什么用悲催这个词？ 如果一个人生下来就在山沟沟里，一辈子都没机会去见什么好东西，这不叫悲催，这只叫苦难；而如果一个人生出来有一个奇怪的特异功能：皮肤出来的汗水会凝结成昂贵的水晶，本来只靠出汗就能赚钱，结果这傻逼居然觉得出汗这个行为太低级，做手术把自己的汗腺全给切了，而且丝毫没有意识到他做了什么傻事，这才叫真的悲催。我们IT界中的很多人，生下来就是有这个出汗成水晶的特异功能的，正是因为这种与众不同，这群人能混入牛逼的大学，整天打网游还能写出像样的毕业论文， 拿到学位，进外企，考CPA，做咨询、做证券分析，研究高分子材料，做电子商务，做云计算……一级一级的上升，直到有一天，发现身边的人里，已经没有一个不是CPA，不是咨询师，不是高级研究员了，身边的人全是业界精英，个个都超级强悍。在这个所谓的高级圈子里，自己并没有任何过人之处，只不过是just another analyst而已。在高级圈子里拼的头破血流，最后也只能混到给台湾人整理数据而已。莫然回首，发现当年的血气方刚、年少时的无限梦想，进化成了一身肥胖的赘肉。这个时候，有个旁观者说：“升级到头了，该降级了”当一个社会疯狂鼓吹快节奏的时候，一定需要有人来宣扬慢生活；当全社会跟打了鸡血似的吹捧升级的时候，一定需要有人来说说降级论。IT青年们喜欢打游戏，喜欢升级。他们的人生也和游戏一样，沉醉于不停的升级中，不仅喜欢升级自己手上的技术，把MySql改成MongoDB，把Apache升级为Nginx，在Mac上装Ubuntu，Ubuntu里再装个虚拟机去跑Mac OS……IT青年们也喜欢升级自己的人生，从程序员升级到项目经理，再升级到技术总监或产品总监，再升级到合伙人……在不断追求升级的过程中，所面临的一个很大事实是：当一个人从A刚升级到A+级的时候，其实这个人的能力层级依然只是A的层级，还未胜任A+的层级，他必须要到A+的后期，才可以胜任A+。就好像一个高中生，高考完之后，虽然理论上已经属于大学生了，但是他的实际能力依然只是高三毕业的水平，除非他全部pass了大一的期末考试。同样的道理，这个世界上有很多人的身份和称谓，都是在描述“未来的自己”，而不是现在的自己。当你从销售员升级为销售经理的时候，你自我感觉很好：“我现在是销售经理了”，但是这个时候 ，你并未通过公司对你作为销售经理这一年的工作成果的考核，你只是一个“未来可能是合格的销售经理”的前身。如果年终考核你失败了，那么这一年最准确的描述是：一个销售员，占了整整一年销售经理的位子，最后失败了。而且这一年一定会过的很累，因为通过考核的其他销售经理，才是真正胜任这个层级的人，跟一帮真正属于这个圈子的人厮杀，就好像拳击馆里当陪练的小角色，去和泰森比了一年的武，怎么可能不累呢？当我07年进入互联网行业的时候，就是那个拳击馆里陪练的小角色，我被迫去跟全国各地的泰森比拼，结果累的半死。后来我开始反思最初的目标，为什么要在自己身上挂一个“拳击高手”的招牌，被那么多泰森追着打？ 我把这块招牌卸了，找个完全没练武的人去比拼，不是更容易赢么？于是果断照做，去找了一个没人懂拳击的小乡村，做了纯英文的Tucia.com(需翻墙)，只做国外的业务。在那个地方，作为一个知名武馆的拳击小陪练，我成了村子里拳击技术最高超的人，受人仰慕，还开武馆教人拳击，活的非常滋润，而且在教人拳击的过程中，自己的拳术也比以前提高了很多，发展出一套属于自己的拳法，我虽然进不了泰森们的大圈子，但他们也进不了我的小圈子。关于圈子，有一个很赤裸裸的现实：不会是你进入圈子，只能是圈子进入你。很多人会四处找关系，“帮我介绍给xxx吧，我想进入你们的圈子”，这样的人是永远进不去这个圈子的，因为圈子的天性是，永远追求更高一个层级的人。而我们的大部分人，其实都在以低一级的属性，占着更高一级的位子，徘徊在更高一级的圈子边缘，与更高一级的人竞争，幻想着自己可以升级到那个圈子里去。也许永远进不去，悲催的努力一辈子；也许运气好，某一天真的进入这个圈子了，但那个时候又会有下一个目标，希望进入更高级的圈子，这是一场没有终点的战斗。永远的追求升级，永远的累。有没有想过降级呢？如果一个来自微软的高级工程师，辞职去一个养猪场做开放平台经理，那么他的到来不仅会让养猪圈感到无比荣幸，更是意味着，利用他在IT界训练出来的高效工作方式和逻辑思维能力，他可以掀起一场养猪行业的革命，使得20年后才会出现的人性、高效、开放、协作、健康的养殖方式提前到达。在这场革命中，他会活的非常有价值。这种价值，在原先的圈子里，是完全体验不到的，因为他此前的所有工作，只是在满身疮痍的windows系统上不停的打补丁，无论打多少都逃不开产品衰落、被人鄙视的命运。很多人的命运，都像是上面那个微软工程师。只需要降级，就能创造更大的价值，也能获得更大的满足。那为什么不呢？为什么要死死抱着那个所谓的“高级职业”不放呢？去年我曾犯贱去趟了移动互联网的浑水，做了个手机app，刚开始的时候感觉很高级，但很快，铺天盖地的竞争对手就出现了，我又发现自己陷入了07年一样的场景：作为一个小小陪练，我他妈的又被一帮泰森们给围住了。当泰森中的战斗机—微信，变得无比牛逼之后，我就知道，战胜这群泰森是绝对不可能的事情了。于是我再次投靠了“降级论”，把自己从牛逼哄哄的移动互联网行业，降级到了一个被人不齿的低级项目：Tucia Baby。这个项目虽然是传统行业，但是我们基本上是按照互联网产品的思路去做的，除了拍摄需要来店里以外，其他一切，包括营销、预约、客服、后期、选片、取片、客户关系等，所有环节都放在网络上。当然，最重要的是，作为一个脑残的果粉，我按照iPhone的做工品质去要求每一张作品，必须达到我们能力可以做到的最好水准，不计成本的最好水准，才允许送给客户。正式接客不到两个月时间，虽然还不敢自称成功，但目前已做到几乎每天都客满，口碑很好，财务上已实现盈利，未来相信一定会比大部分app开发者更光明。（ps:我们没有请工商、税务、城管去吃饭喝酒泡桑拿，也没有塞钱给任何政府机关。当你的产品真的用心做到很好的时候，其实你不需要讨好任何人的。）这个项目让我沉思了很久：07年我曾把一个纯纯的web2.0网站做到了alexa中国区前1000名（如有质疑，请查询2010年附近的tucia.com排名，那时候还是一个中文网站），结果一路亏损，到最后只剩下一个员工；11年我把那个纯纯的app做到苹果官方推荐区免费榜的第一位（截图），那段时间每天四五千iPhone安装量，结果一路烧钱，到最后濒临关闭；而如今，我只需把自己从纯纯的互联网降级下来，做一些看起来有些“低级”的项目，居然就能立即实现收支平衡。除此以外，我还发现一个现象，中国消费者在与奸商们的长期斗争中，已经培养出了一种非常苦B的品质：只要不被坑，他就谢天谢地。如果商家严格做到了承诺的每一件事情，客户就会感动的泪如泉涌。如果商家不仅做到了所有承诺的事情，还很贴心的提供了一些额外的服务（比如我们给每位客户赠送非常好吃的樱桃和昂贵的进口巧克力作为点心），那么客户就会激动的哭天喊地、奔走相告，推荐给他认识的每一个人。其实这片肮脏的国土，就是上天赐予IT青年们的最好机会。在一个不会练武的村子里，只要你会打两拳，你就是拳术最厉害的人；在一个没有服务意识、忽视产品质量的土地上，只要你用心做服务，用最高的标准去要求自己，你就会成为这块土地上最出色的商家；在一个没有现代管理意识，不懂网络、不懂微博、不懂用户体验、不懂口碑传播的粗犷社会里，你只需要把之前花在IT产品上的心思的10%拿过来用，就可以秒杀一切天朝对手。所以，IT青年们，当你在为网站的转化率苦苦思索的时候，当你在为app的活跃度辗转反侧的时候，当你在为融资计划苦苦哀求各界大佬引荐的时候，也许犯了一个错误，也许你们的脑子最值得闪光的地方，不是去悲催的IT界当炮灰，而应该是去按摩界、餐饮界、烧烤界、早餐界、理发界、送花界、纺织界、装修界、婚庆葬仪界、成人用品界、现代养殖界、有机蔬果界、个人护理界、汽车修理界……与IT界相比，这些行业的确无比低级，他们的老板连qq都会发音成“抠抠”，他们的员工一辈子都没用过Email；跟他们解释什么是SEO，什么是用户体验，什么是数据挖掘，他们会在听你说完之前就开枪自杀掉。正是因为如此，这些行业才是如此的不堪一击。正是因为如此，当智商高达147的IT青年还在为3k薪水拼命、而智商不到50的烧烤店老板正坐在porsche里玩着前面那位青年开发的app的时候，我就忍不住仰望星空。这些原始而纯粹的行业，正在等待IT精英们的降级，如同蒲公英一般的伞兵，在黑夜里从天而降，长驱直入，用最智慧的产品、最优质的服务拯救这些早就该死的行业，屌丝的生命将会绽放出银色的羽翼，无比丰满，无比性感。参考本文章来源于网络]]></content>
      <categories>
        <category>其他阅读</category>
      </categories>
      <tags>
        <tag>好文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql事物隔离级别和传播行为]]></title>
    <url>%2F2018%2Fmysql-transaction.html</url>
    <content type="text"><![CDATA[事务隔离级别事务隔离级别的语义：当前事务执行过程中，通过select，update，delete 操作，对其他事务的影响，反过来也是如此，通俗的说就是 当前事务是否可以看到其他事务的操作结果。数据库事务的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。mysql 默认的隔离级别是：Repeatable read如何查询当前数据库的隔离级别select @@tx_isolation;SELECT @@session.tx_isolation;SELECT @@global.tx_isolation;设置set tx_isolation=’read-committed’;不同隔离级别的影响ANSI/ISO SQL标准定义了4中事务隔离级别：未提交读（read uncommitted），提交读（read committed），重复读（repeatable read），串行读（serializable）。对于不同的事务，采用不同的隔离级别分别有不同的结果。不同的隔离级别有不同的现象。主要有下面3种现在：1、脏读（dirty read）：一个事务可以读取另一个尚未提交事务的修改数据。2、不可重复读（nonrepeatable read）：在同一个事务中，同一个查询在T1时间读取某一行，在T2时间重新读取这一行时候，这一行的数据已经发生修改，可能被更新了（update），也可能被删除了（delete）。3、幻像读（phantom read）：在同一事务中，同一查询多次进行时候，由于其他插入操作（insert）的事务提交，导致每次返回不同的结果集。不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差，4种事务隔离级别分别表现的现象如下图：注意：1、repeatable read 允许幻读，这是ANSI/ISO SQL标准的定义要求，运行幻读依然有非常大的隐患，mysql innodb引擎 在repeatable read 即可满足没有幻读的要求。2、不可重复读和幻读的区别：不可重复读的重点是修改，幻读的重点是插入或者删除了新数据。两都会造成系统错误，但是避免的方法则区别比较大，对于前者, 只需要锁住满足条件的记录，对于后者, 要锁住满足条件及其相近的记录。Read uncommitted这是事务最低的隔离级别，它充许令外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。(1)所有事务都可以看到其他未提交事务的执行结果(2)本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少(3)该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#首先，修改隔离级别set tx_isolation='READ-UNCOMMITTED';select @@tx_isolation;+------------------+| @@tx_isolation |+------------------+| READ-UNCOMMITTED |+------------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在事务B中执行更新语句，且不提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：那么这时候事务A能看到这个更新了的数据吗?select * from tx;+------+------+| id | num |+------+------+| 1 | 10 | ---&gt;可以看到！说明我们读到了事务B还没有提交的数据| 2 | 2 || 3 | 3 |+------+------+#事务B：事务B回滚,仍然未提交rollback;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务A：在事务A里面看到的也是B没有提交的数据select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;脏读意味着我在这个事务中(A中)，事务B虽然没有提交，但它任何一条数据变化，我都可以看到！| 2 | 2 || 3 | 3 |+------+------+ Read committed保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据(1)这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）(2)它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变(3)这种隔离级别出现的问题是——不可重复读(Nonrepeatable Read)：不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。|——&gt;导致这种情况的原因可能有：(1)有一个交叉的事务有新的commit，导致了数据的改变;(2)一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#首先修改隔离级别set tx_isolation='read-committed';select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| READ-COMMITTED |+----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在这事务中更新数据，且未提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：这个时候我们在事务A中能看到数据的变化吗?select * from tx; ---------------&gt;+------+------+ || id | num | |+------+------+ || 1 | 1 |---&gt;并不能看到！ || 2 | 2 | || 3 | 3 | |+------+------+ |——&gt;相同的select语句，结果却不一样 |#事务B：如果提交了事务B呢? |commit; | |#事务A: |select * from tx; ---------------&gt;+------+------+| id | num |+------+------+| 1 | 10 |---&gt;因为事务B已经提交了，所以在A中我们看到了数据变化| 2 | 2 || 3 | 3 |+------+------+ Repeatable read这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。(1)这是MySQL的默认事务隔离级别(2)它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行(3)此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行(4)InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#首先，更改隔离级别set tx_isolation='repeatable-read';select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：开启一个新事务(那么这两个事务交叉了) 在事务B中更新数据，并提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+commit;#事务A：这时候即使事务B已经提交了,但A能不能看到数据变化？select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;还是看不到的！(这个级别2不一样，也说明级别3解决了不可重复读问题)| 2 | 2 || 3 | 3 |+------+------+#事务A：只有当事务A也提交了，它才能够看到数据变化commit;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+ Serializable这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。(1)这是最高的隔离级别(2)它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。(3)在这个级别，可能导致大量的超时现象和锁竞争123456789101112131415161718#首先修改隔离界别set tx_isolation='serializable';select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| SERIALIZABLE |+----------------+#事务A：开启一个新事务start transaction;#事务B：在A没有commit之前，这个交叉事务是不能更改数据的start transaction;insert tx values('4','4');ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionupdate tx set num=10 where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 事物传播行为PROPAGATION_REQUIRED如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 PROPAGATION_SUPPORTS支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘ PROPAGATION_MANDATORY支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 PROPAGATION_REQUIRES_NEW创建新事务，无论当前存不存在事务，都创建新事务。 PROPAGATION_NOT_SUPPORTED以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 Mysql(Innodb)如何避免幻读幻读Phantom Rows幻读问题是指一个事务的两次不同时间的相同查询返回了不同的的结果集。例如:一个 select 语句执行了两次，但是在第二次返回了第一次没有返回的行,那么这些行就是“phantom” row.read view(或者说 MVCC)实现了一致性不锁定读(Consistent Nonlocking Reads)，从而避免了幻读 浅谈mysql mvccmysql的大多数事务型存储引擎实现的都不是简单的行级锁，基于提升并发性能的考虑，他们一般都同时实现了多版本并发控制，可以认为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低，虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。MVCC的实现是通过保存数据在某个时间点的快照来实现的，也就是说，不管需要执行多长时间，只要事务开始时间相同，每个事务看到的数据都是一致的，事务开始的时间不同时，每个事务对同一张表，同一时刻看到的数据可能是不一样的（因为不同的时间点可能数据就已经产生了不同的快照版本，而每个事务在默认的RR隔离级别下只能看到事务开始时的数据快照）。说道不同的存储引擎的MVCC实现是不同的，典型的有乐观并发控制和悲观并发控制，下面简单说明MVCC是如何工作的：例如：此时books表中有5条数据，版本号为1事务A，系统版本号2：select from books；因为1&lt;=2所以此时会读取5条数据。事务B，系统版本号3：insert into books …，插入一条数据，新插入的数据版本号为3，而其他的数据的版本号仍然是2，插入完成之后commit，事务结束。事务A，系统版本号2：再次select from books；只能读取&lt;=2的数据，事务B新插入的那条数据版本号为3，因此读不出来，解决了幻读的问题。MVCC只在repeatable-read和read-committed两个隔离级别下才工作，其他两个隔离级别都和MVCC不兼容，因为read uncommitted总是读取最新的数据行，而不是符合当前事务版本的数据行，而serializeble则会对所有读取的行都加锁。另外要注意：MVCC在RR和RC隔离级别下的区别，在RR隔离级别下，一个事务只能读取到事务开始的那个时刻的数据快照，即，别的事务修改并提交的数据在自身没有提交之前一般读取不到（加for update语句的select除外，因为这个语句要对数据加X锁必须读取最新的数据快照）， 在RC隔离级别下，事务总是读取数据行的最新快照，即会产生不可重复读的问题。repeatable-read 解决幻读还有一个特烈就是这个查询可以看到于自己开始之后的同一个事务产生的变化，这个特例会产生一些反常的现象。repeatable-read 幻读特例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071SESSION_A开始事务并创建快照SESSION_A&gt;START TRANSACTION WITH CONSISTENT SNAPSHOT;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)SESSION_B&gt;insert into read_view values('anomaly'),('anomaly');Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0SESSION_B&gt;update read_view set text='INIT' where text='init';Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)SESSION_A更新了它并没有"看"到的行SESSION_A&gt;update read_view set text='anomaly!' where text='anomaly';Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select || anomaly! || anomaly! |+-------------------------+5 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| INIT || after session A select || before Session_A select || anomaly! || anomaly! |+-------------------------+5 rows in set (0.00 sec) 观察实验步骤可以发现，在倒数第二次查询中，出现了一个并不存在的状态这里A的前后两次读，均为快照读，而且是在同一个事务中。但是B先插入直接提交，此时A再update，update属于当前读，所以可以作用于新插入的行，并且将修改行的当前版本号设为A的事务号，所以第二次的快照读，是可以读取到的，因为同事务号。这种情况符合MVCC的规则，如果要称为一种幻读也非不可，算为一个特殊情况来看待吧。 深层次的原理分析在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。 快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。 当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以MySQL InnoDB为例： 快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析)select * from table where ?; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。select from table where ? lock in share mode;select from table where ? for update;insert into table values (…);update table set ? where ?;delete from table where ?;所有以上的语句，都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。 MySQL/InnoDB定义的4种隔离级别： Read Uncommited可以读取未提交记录。此隔离级别，不会使用，忽略。 Read Committed (RC)快照读（普通的select就是快照读）是不会对读取的行加锁的，所以存在幻读现象。 针对当前读，RC隔离级别保证对读取到的记录加锁 (记录锁)，加锁后的内容不允许update和delete，但是可以新增，所以存在幻读现象。例如：123456事物1 SELECT * from qq for update ; 5条记录事物2 delete from qq limit 1; commit； 会阻塞 因为上面一条语句有记录锁事物1update qq set bb=2 ; commit；会更新5条 然后事物2执行完毕删除一条 例如存在幻读现象：123456事物1 SELECT * from qq for update ; 5条记录事物2 insert into qq values(123456); commit；不会阻塞事物1update qq set bb=2 ; commit；会更新6条 Repeatable Read (RR)快照读（普通的select就是快照读）是不会对读取的行加锁的，所以此情况还是会存在幻读现象。 针对当前读，RR隔离级别保证对读取到的记录加锁 (记录锁)，同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入 (间隙锁)，不存在幻读现象。例如：123456事物1 SELECT * from qq for update ; 5条记录事物2 insert into qq values(123456); commit；会阻塞 因为上面一条语句有间隙锁事物1update qq set bb=2 ; commit；会更新5条 然后执行事物2 新增一条 例如：123456事物1 SELECT * from qq for update ; 5条记录事物2 delete from qq limit 1; commit； 会阻塞 因为上面一条语句有记录锁事物1update qq set bb=2 ; commit；会更新5条 然后事物2执行完毕删除一条 Serializable从MVCC并发控制退化为基于锁的并发控制。不区别快照读与当前读，所有的读操作均为当前读，读加读锁 (S锁)，写加写锁 (X锁)。 Serializable隔离级别下，读写冲突，因此并发度急剧下降，在MySQL/InnoDB下不建议使用。 InnoDB通过Nextkey lock解决了当前读时的幻读问题Innodb行锁分为: 类型 说明Record Lock: 在索引上对单行记录加锁.Gap Lock: 锁定一个范围的记录,但不包括记录本身.锁加在未使用的空闲空间上,可能是两个索引记录之间，也可能是第一个索引记录之前或最后一个索引之后的空间.Next-Key Lock: 行锁与间隙锁组合起来用就叫做Next-Key Lock。锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。实验1创建表123456(mysql@localhost) [fandb]&gt; create table t5(id int,key(id));Query OK, 0 rows affected (0.02 sec)SESSION_A&gt;insert into t5 values(1),(4),(7),(10);Query OK, 4 rows affected (0.00 sec)Records: 4 Duplicates: 0 Warnings: 0 开始实验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 4 || 7 || 10 |+------+4 rows in set (0.00 sec)SESSION_A&gt;select * from t5 where id=7 for update;+------+| id |+------+| 7 |+------+1 row in set (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t5 values(2);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t5 values(12);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t5 values(5); --被阻塞^CCtrl-C -- sending "KILL QUERY 93" to server ...Ctrl-C -- query aborted.^[[AERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;insert into t5 values(7); --被阻塞^CCtrl-C -- sending "KILL QUERY 93" to server ...Ctrl-C -- query aborted.ERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;insert into t5 values(9); --被阻塞^CCtrl-C -- sending "KILL QUERY 93" to server ...Ctrl-C -- query aborted.ERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 4 || 7 || 10 |+------+4 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 2 || 4 || 7 || 10 || 12 |+------+6 rows in set (0.00 sec) 当以当前读模式select * from t5 where id=7 for update;获取 id=7的数据时,产生了 Next-Key Lock,锁住了4-10范围和 id=7单个record从而阻塞了 SESSION_B在这个范围内插入数据，而在除此之外的范围内是可以插入数据的。在倒数第二个查询中,因为 read view 的存在，避免了我们看到 2和12两条数据，避免了幻读同时因为 Next-Key Lock 的存在,阻塞了其他回话插入数据，因此当前模式读不会产生幻读(select for update 是以当前读模式获取数据) 尽量使用唯一索引,因为唯一索引会把Next-Key Lock降级为Record Lock实验2创建表123456(mysql@localhost) [fandb]&gt; create table t6(id int primary key); Query OK, 0 rows affected (0.02 sec)SESSION_A&gt;insert into t6 values(1),(4),(7),(10); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 开始实验12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 7 || 10 |+----+4 rows in set (0.00 sec)SESSION_A&gt;select * from t6 where id=7 for update;+----+| id |+----+| 7 |+----+1 row in set (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t6 values(5); --插入成功没有阻塞Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t6 values(8); --插入成功没有阻塞Query OK, 1 row affected (0.00 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 7 || 10 |+----+4 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 5 || 7 || 8 || 10 |+----+6 rows in set (0.00 sec) 当 id 列有唯一索引,Next-Key Lock 会降级为 Records Lock InnoDB引擎的行锁和表锁行锁和表锁在mysql 的 InnoDB引擎支持行锁，与Oracle不同，mysql的行锁是通过索引加载的，即是行锁是加在索引响应的行上的，要是对应的SQL语句没有走索引，则会全表扫描，行锁则无法实现，取而代之的是表锁。表锁：不会出现死锁，发生锁冲突几率高，并发低。行锁：会出现死锁，发生锁冲突几率低，并发高。锁冲突：例如说事务A将某几行上锁后，事务B又对其上锁，锁不能共存否则会出现锁冲突。（但是共享锁可以共存，共享锁和排它锁不能共存，排它锁和排他锁也不可以）死锁：例如说两个事务，事务A锁住了1~5行，同时事务B锁住了6~10行，此时事务A请求锁住6~10行，就会阻塞直到事务B施放6~10行的锁，而随后事务B又请求锁住1~5行，事务B也阻塞直到事务A释放1~5行的锁。死锁发生时，会产生Deadlock错误。锁是对表操作的，所以自然锁住全表的表锁就不会出现死锁。 行锁的类型行锁分 共享锁 和 排它锁。 共享锁（Shared Lock，也叫S锁）表示对数据进行读操作。因此多个事务可以同时为一个对象加共享锁，对于同一数据都能访问到数据，但是只能读不能修改。产生共享锁的sql：select from ad_plan lock in share mode;共享锁的使用场景 SELECT … LOCK IN SHARE MODE走的是IS锁(意向共享锁)，即在符合条件的rows上都加了共享锁，这样的话，其他人可以读取这些记录，也可以继续添加IS锁，但是无法修改这些记录直到你这个加锁的过程执行完成(完成的情况有：事务的提交，事务的回滚，否则直接锁等待超时)。 SELECT … LOCK IN SHARE MODE的应用场景适合于两张表存在关系时的写操作，拿mysql官方文档的例子来说，一个表是child表，一个是parent表，假设child表的某一列child_id映射到parent表的c_child_id列，那么从业务角度讲，此时我直接insert一条child_id=100记录到child表是存在风险的，因为刚insert的时候可能在parent表里删除了这条c_child_id=100的记录，那么业务数据就存在不一致的风险。正确的方法是再插入时执行select from parent where c_child_id=100 lock in share mode,锁定了parent表的这条记录，然后执行insert into child(child_id) values (100)就不会存在这种问题了。 排他锁(Exclusive Lock，也叫X锁)排他锁又称为写锁，简称X锁，顾名思义，排他锁就是不能与其他所并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。获取排他锁的事务是可以对数据就行读取和修改。mysql InnoDB引擎默认的修改数据语句，update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型。产生排他锁的sql： select * from ad_plan for update;排他锁的使用场景： 使用场景一：订单的商品数量 但是如果是同一张表的应用场景，举个例子，电商系统中计算一种商品的剩余数量，在产生订单之前需要确认商品数量&gt;=1,产生订单之后应该将商品数量减1。 1 select amount from product where product_name=’XX’; 2 update product set amount=amount-1 where product_name=’XX’; 显然1的做法是是有问题，因为如果1查询出amount为1，但是这时正好其他session也买了该商品并产生了订单，那么amount就变成了0，那么这时第二步再执行就有问题。那么采用lock in share mode可行吗，也是不合理的，因为两个session同时锁定该行记录时，这时两个session再update时必然会产生死锁导致事务回滚。以下是操作范例(按时间顺序) 使用场景一：数据表的状态 如果存在一张表记录一个商品的状态，在订单的变化过程中，订单的状态是不断变化的，而且变化的过程中肯定也会有并发的问题，而且很多时候与其他系统有交互，会存在补偿的情况，所以并发的可能性很大,补偿或者为了增加状态修改的成功可能性，2次改变状态的情况也有，楼主就遇到了这种情况，真操蛋。于是看到有这样的for update写法。 1 update order set status = 1 where product_id = ‘1’; 2 insert order_flow (…………..) value (………) 这样的情况下就有可能订单的状态已经更新完成了，但是补偿这些额外的消息把状态又更新为待处理或者插入了多条流水的情况(多条流水的可能性大，状态的那种可能补偿滞后)。这个时候就可以使用select …. from order where order_id = ‘1’ for update，先锁住要修改状态的表，这样就不会别人操作了，自己先后面把流水插入，然后更新状态，完美。但是加了锁之后性能就很慢了，担心性能影响，而且有可能存在死锁的情况，后面我就修改为流水表中增加一个唯一索引，这样插入流水报错就是已经处理过的记录了。这样就不会存在性能问题。 通过对比，lock in share mode适用于两张表存在业务关系时的一致性要求，for update适用于操作同一张表时的一致性要求。 行锁注意几点1.行锁必须有索引才能实现，否则会自动锁全表，那么就不是行锁了。2.两个事务不能锁同一个索引，例如：123456事务A先执行：select math from zje where math&gt;60 for update;事务B再执行：select math from zje where math&lt;60 for update；这样的话，事务B是会阻塞的。如果事务B把 math索引换成其他索引就不会阻塞，但注意，换成其他索引锁住的行不能和math索引锁住的行有重复。 3.insert ，delete ， update在事务中都会自动默认加上排它锁。实现：1234567会话1：begin；select math from zje where math&gt;60 for update；会话2：begin；update zje set math=99 where math=68；阻塞........... 会话相当与用户如上，会话1先把zje表中math&gt;60的行上排它锁。然后会话2试图把math=68的行进行修改，math=68处于math&gt;60中，所以是已经被锁的，会话2进行操作时，就会阻塞，等待会话1把锁释放。当commit时或者程序结束时，会释放锁。 mysql死锁所谓死锁: 是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程.表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB.死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。那么对应的解决死锁问题的关键就是：让不同的session加锁有次序死锁例子：下面两个事物都执行了第一条update语句，更新了该数据同事锁定了该行数据接着每个事物都去执行第二天语句，却发现都被对方锁定，进入死循环。123456789事物1start TRANSACTIONupdate STOCKPRICE SET close = 89 where stock_id =4 and date = '2002-05-01'update STOCKPRICE SET close = 22 where stock_id =3 and date = '2002-05-02'事物2start TRANSACTIONupdate STOCKPRICE SET close = 11 where stock_id =3 and date = '2002-05-02'update STOCKPRICE SET close = 33 where stock_id =4 and date = '2002-05-01' 死锁解决办法：数据库实现了个各种死锁的检测和超时机制。InnoDb可以检测到死锁的循环依赖，并返回一个错误，另一个就是查询时间达到锁超时时间设定后自动放弃锁请求。 如何尽可能避免死锁1）以固定的顺序访问表和行。比如对第2节两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；又比如对于3.1节的情形，将两个事务的sql顺序调整为一致，也能避免死锁。 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 死锁查询1、查询是否锁表show OPEN TABLES where In_use &gt; 0; 2、查询进程 show processlist 查询到相对应的进程===然后 kill id 3、查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 4、查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql查询性能优化]]></title>
    <url>%2F2018%2Fmysql-select.html</url>
    <content type="text"><![CDATA[优化数据访问是否请求了不需要的数据有些查询会请求超过实际需要的数据，然后多余的数据会被应用程序丢弃。这会给mysql服务器带来额外的负担，并增加网络开销，另外也会消耗应用服务器的CPU和内存资源。比如：1、查询不需要的记录实际页面显示10条记录，但是查询去除100条记录，解决办法查询后面使用limit 10.2、多表关联时返回全部列如果只需要出现的演员，千万不要按照下面的写法：1select * from actor inner join film_actor using(actor_id) inner join film using(film_id) where film.title = 'Academy'; 这将返回这三个表的全部数据列。正确的写法如下：1select actor.* from actor inner join film_actor using(actor_id) inner join film using(film_id) where film.title = 'Academy'; 3、总是取出全部列使用select * from table 的时候需要想一想所有的列是否都是需要的，尽量抛弃这种写法4、重复查询相同的数据不断地重复执行相同的查询，然后每次返回完全相同的数据。比较好的方案是查询的时候将这个数据缓存起来，需要的时候从缓存中取出。 mysql是否在扫描额外的记录mysql最简单的衡量查询开销的三个指标：响应时间、扫描行数、返回行数。没有哪个指标能够完美的衡量查询的开销，但它们大致反映了mysql在内部执行查询时需要访问多少数据，并可以大概推算出查询运行的时间。这三个指标都会记录到mysql的慢日志中，所以检查慢日志记录是找出扫描数过多的查询的好办法。1、响应时间响应时间只是一个表面上的值。响应时间是服务时间和排队时间的和。服务时间是指数据库处理这个查询真正花了多长时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间–可能是等I/O操作完成，可能是等待行锁等等。当看到一个查询的响应时间的时候首先问问自己这个响应时间是否是一个合理的值。2、扫描的行数和返回的行数在一定程度上能说明该查询找到需要的数据的效率高不高。对于糟糕的查询，并不是所有的行的访问代价都是相同的，较短行的访问速度更快，因为内存中的行比磁盘中的行的访问速度要快得多。理想情况扫描的行数和返回的行数应该是相同的。 扫描的行数和访问类型在explain语句中的type列反应了访问类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。如果查询没有办法找到合适的访问类型，那么解决办法是加一个合适的索引，请参考另一篇文章mysql高性能索引 重构查询方式目标是找到一个更优的方法活的实际需要的结果–不一定总是需要mysql获取一模一样的结果集，有时候可以将查询转换一种写法让其返回一样的结果，但性能更好。 一个复杂查询还是多个简单查询设计查询的时候需要考虑的问题是否需要将一个复杂的查询分成多个简单的查询。以往的逻辑认为网络通信查询解析和优化是一件代价很高的事情。但是mysql并不适用，mysql从设计上让连接和断开都是很轻量级，返回一个小的查询结果方面很高效，现代的网络速度也比以前快很多。不过在设计的时候如果一个查询能够胜任的时候还写成多个独立查询是不明智的。 切分查询有时候对于一个大的查询需要分而治之，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分查询结果。例如删除一个月前的日志信息，数据量很大，可以分批次一次删除一万行数据，一般来说一万行数据是一个比较高效且对服务器影响也是最小的做法。同时需要注意的是每次删除之后暂停一会儿再做下次删除，将服务器压力分散到一个很长的时间中，可以大大降低服务器的影响，还可以减少删除时锁的持有时间。 分解关联查询可以对每一个表进行一次单表查询，然后将结果在应用程序中进行关联。分解关联查询的方式重构查询有如下优势：1、让缓存的效率更高。2、减少锁竞争。3、在应用层做关联，可以容易对数据库进行拆分，更容易做到高性能和扩展。4、查询本身效率也可能会有所提升。5、可以减少冗余数据的查询。 查询执行的基础 mysql客户单/服务器通信协议mysql客户单和服务器之间的通信协议是半双工的，这意味着在任何一个时刻，要么是由服务器向客户端发送数据，要么是由客户单向服务器发送数据，这两个动作不能同时发生。客户端用一个单独的数据包将查询传给服务器，这也是为什么当查询的语句很长的时候，参数max_allowed_packet就特别重要了。一旦客户端发送请求就只能等待结果了。相反的一般服务器响应给用户的数据通常很多，有多个数据包组成，当服务器开始响应客户端请求时，客户端必须完整的接受整个返回结果，而不能单独的只取前面的几条结果，然后让服务器停止发送请求。mysql通常需要等所有的数据都已经发送给客户端才能释放这条查询所占用的资源，让查询早点结束可以早点释放响应的资源。 查询状态对于一个mysql连接，或者说一个线程，任何时刻都有一个状态，该状态标识了mysql当前正在做什么。show full processlist命令查看当前的状态。Sleep线程正在等待客户端发送新的请求。Query线程正在执行查询或者正在将结果发送给客户端。Locked在mysql服务器层，该线程正在等待表锁。在存储引擎级别实现的锁，例如InnoDB的行锁并不会体现在线程状态中。Analyzing and statistics线程正在收集存储引擎的统计信息，并生成查询的执行计划。Copying to tmp table [on disk]线程正在执行查询，并且将结果集都复制到一个临时表中，这种状态一般要么是在做group by操作，要么是文件排序操作，或者是union操作。如果这个状态后面还有 on disk 标记，那么标识mysql正在将一个内存临时表放到磁盘上。Sorting result线程正在对结果集进行排序。Sending data线程可能在多个状态之间传送数据，或者在生成结果集，或者在向客户端返回数据。了解这些状态的含义非常有用，在一个繁忙的服务器上可能会看到大量的不正常的状态，例如statistics正占用大量的时间。通常表示某个地方有异常了。可以看mysql服务器性能剖析来诊断哪个环节出现问题了。 查询缓存如果查询缓存是打开的，mysql会优先检查这个查询是否命中查询缓存中的数据，这个检查是通过一个对大小写敏感的哈希查找实现的。如果命中了查询缓存，在返回结果之前mysql会检查一次用户权限。 查询优化处理语法解析器和预处理mysql通过关键字将sql语句进行解析，并生成一颗对应的解析树，mysql解析器将使用mysql语法规则验证和解析查询。预处理器则根据一些mysql规则进一部检查及解析树是否合法：表和数据列是否存在，别名是否有歧义，下一步预处理器会验证权限。 查询优化器mysql使用基于成本的优化器，它会尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。可以通过查询当前回话的Last_query_cost的值来得知mysql计算当前查询的成本。执行查询 select sql_no_cache count(*) from actor; 后执行 show status like ‘Last_query_cost’;结果value值标识mysql的优化器任务大概需要做多少个数据页的随机查询才能完成上面的查询。有多重原因导致mysql有乎其选择错误的执行计划，如下所示:1、统计信息不准确。2、执行计划汇总的成本估算不等同于实际执行的成本。3、mysql最优可能和你像的最优不一样。4、mysql从不考虑其他并发执行的查询，这可能会影响当前查询速度。5、mysql不会考虑不收其控制的操作成本。例如存储过程或者用户自定义的函数的成本。6、优化器有时候无法估算所有可能的执行计划，所以可能错过最优的执行几环。 mysql如何执行关联查询mysql认为任何一个查询都是一次“关联”，并不是一次查询需要两张表才叫关联查询。对于union查询，mysql先将一系列的单个查询结果放到一个临时表中，然后重新读出临时表数据来完成union查询。mysql任何关联都执行嵌套循环关联操作，即mysql先在一个表中循环取出单条数据，然后再嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为止。然后根据各个表匹配的行，返回查询中需要的各个列。 执行计划mysql并不会生成查询字节码来执行查询。mysql生成查询的一颗指令树，然后通过存储引擎执行完成这棵指令树并返回结果，最终的执行计划包含了重构查询的全部信息。如果对某个查询执行explain extended 后，在执行 show warnings 就可以看到重构出的查询。 关联查询优化器mysql优化器最重要的一部分就是关联查询优化，它决定了多个关联表时的顺序。通常多表关联查询的时候，可以有多重不同的关联顺序来获取相同的执行结果。关联查询优化器则通过评估不同顺序时的成本来选择一个代价最小的关联顺序。绝大多数情况下优化都是有效的，但因为不会去计算每一种关联顺序的成本，所以偶尔也会选择一个不是最优的执行计划。 排序优化无论如何排序都是一个成本很高的操作，所以从性能考虑，尽可能避免排序或者尽可能避免大量数据进行排序。当不能使用索引生成排序结果的时候，mysql需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，不过mysql将这个过程统一称为文件排序filesort，即使完全是内存排序。mysql在进行排序的时候需要使用的临时存储空间可能会比想象的要大得多，因为mysql在排序时，对每一个排序记录都会分配一个足够长的定长空间来存放。在关联查询的时候如果需要排序，mysql会分两种情况来处理这样的文件排序。如果order by子句中的所有列都来自关联的第一个表，那么在关联处理第一个表的时候就进行文件排序，如果是这样在explain中的结果会看到Using filesort，除此之外mysql都会先将关联的结果放到一个临时表中，等所有的关联都结束后，在进行文件排序，这样explain会看到 Using temporary;sing filesort。如果查询有limit，limit会在排序之后应用。 查询执行引擎在解析和优化阶段，mysql将生成查询对应的执行计划，mysql的查询执行引擎则根据这个执行计划来完成整个查询。这里执行计划是一个数据结构，而不是字节码。 返回结果给客户端查询执行的最后一个阶段是将结果返回给客户端。即使没有结果也会返回一些信息比如影响到的行数。如果可以被缓存，mysql也会在这个阶段将结果放到查询缓存中。mysql将结果集返回客户端是一个增量、逐步返回的过程。例如一个关联操作，一旦服务器处理完最后一个关联表，开始生成第一条结果时，mysql就可以开始向客户端逐步返回结果集了。这样做的好处是：服务器无需存储太多的结果，而消耗太多内存。可以使用SQL_BUFFER_RESULT来影响整个行为。结果集中的每一行都会以一个满足mysql通信协议的封包发送，再通过tcp协议进行传输，在tcp传输过程中，可能对mysql的封包进行缓存然后批量传输。 mysql查询优化器的局限性mysql的万能“嵌套查询”并不是对每种查询都是最优的。 关联子查询mysql的子查询实现得非常糟糕。最糟糕的是where查询中包含in()的子查询语句，例如：1select * from film where film_id in(select film_id from actor where actor_id = 1); mysql对in()列表中的选项有专门的优化策略，会把上面的查询改写成下面的样子：1select * from film where exists(select * from actor where actor_id=1 and actor.film_id=film.film_id); 这时mysql选择对file表进行全表扫描，然后根据返回的film_id逐个执行子查询，如果外层是个非常大的表，性能会非常糟糕。可以用下面的办法重写这个查询：1select film.* from film inner join actor using(film_id) where actorid =1 union的限制mysql无法将限制条件从外层下推到内层，这使得原本能够限制部分返回结果的条件无法应用到内层查询的优化上。例如：1(select name from actor order by name) union all (select name from customer order by name) limit 20; 上面这条语句会把两个表所有的记录房子啊一个临时表，然后再从临时表取出前20条。可以改写如下：12(select name from actor order by name limit 20) union all (select name from customer order by name limit 20) order by name limit 20; 最大值和最小值优化对于min() 和max()mysql优化做的并不好。例如：1select min(actor_id) from actor where name =‘penelope’; 因为name并没有索引，因此mysql将会进行一次全表扫描。如果mysql能够进行主键扫描那么理论上mysql读到第一个满足条件的记录的时候就是我们需要的最小值了，因为主键actor_id字段的大小顺序是排列的。可以改写如下：1select actor_id from actor use index(primary) where name =‘penelope’ limit 1; 并行执行mysql无法利用多核特性来并行执行查询。 在同一个表上查询和更新mysql不允许对同一张同时进行查询和更新。如下图所示：可以改写成下图所示： 优化特定类型的查询优化count()查询count(*)对行的数目进行计算，包含NULL。count(column)对特定的列的值具有的行数进行计算，不包含NULL值。count()还有一种使用方式，count(1)这个用法和count(*)的结果是一样的。count(distinct col) 计算该列除 NULL 之外的不重复行数，注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。当某一列的值全是 NULL 时，count(col)的返回结果为 0，但 sum(col)的返回结果为 NULL，因此使用 sum()时需注意 NPE 问题。正例:可以使用如下方式来避免sum的NPE问题:SELECT IF(ISNULL(SUM(g)),0,SUM(g)) FROM table;性能问题1.任何情况下SELECT count(*) FROM tablename是最优选择；2.尽量减少SELECT count(*) FROM tablename WHERE COL = ‘value’ 这种查询；3.杜绝SELECT COUNT(COL) FROM tablename WHERE COL2 = ‘value’ 的出现。如果表没有主键，那么count（1）比count(*)快。如果有主键，那么count（主键，联合主键）比count(*)快。如果表只有一个字段，count(*)最快。count(1)跟count(主键)一样，只扫描主键。count(*)跟count(非主键)一样，扫描整个表。明显前者更快一些。 优化关联查询1、确保on或者using子句中的列上有索引，在创建索引的时候就需要考虑到关联的顺序，当表A和表B用到c关联的时候，如果优化器的关联顺序是B、A，那么就不需要在B表的对应列上加索引，没用到的索引只会带俩额外的负担，只需要在关联顺序中的第二个表的相应列上创建索引。2、确保任何的group by和order by中的表达式只涉及到一个表中的列，这样mysql才有可能使用索引来优化这个过程。3、当升级mysql的时候要注意关联语法、运算符优先级等其他可能发生变化的地方。 优化子查询子查询优化给出的最重要的优化建议就是尽可能使用关联查询代替。 优化group by 和distinct如果有索引，它们都可以使用索引来优化，这也是最有效的优化办法。如果没有索引的时候，group by 使用两种策略来完成：使用临时表或者文件排序来做分组。可以使用提示SQL_BIG_RESULT和SQL_SMALL_RESULT来让优化器按照你希望的方式运行。在分组查询的select中直接使用非分组列通常都不是什么好主意。如果没有通过order by子句显式的指定排序列，当查询使用group by子句的时候结果集会自动按照分组的字段进行排序，如果不关心结果集的顺序，则可以使用order by null让mysql不在进行排序。 优化limit 分页优化分页查询的一个最简单的办法就是尽可能地使用索引覆盖扫描，恶如不是查询所有的列。例如下面的查询：1select film_id ,description from film order by title limit 50,5; 如果这个表非常大，最好改成下面的样子：1select film.film_id,film.description from film inner join (select film_id from film order by title limit 50,5) as lim using(film_id); 这里的延迟关联将大大提升查询效率，这个技术也可以优化关联查询中的limit子句。 优化SQL_CALE_FOUND_ROWSlimit语句加上这个提示虽然可以获取分页的总数，但是mysql总会扫描所有满足条件的行，然后在抛弃不需要的行，该代价非常高。一个更好的设计师将页数换成下一页按钮，假设每页20条记录，那么每次查询时都用limit返回21条记录并只显示20条，如果21条存在那么显示下一页按钮否则不显示。有时候也可以考虑使用 explain的结果中的rows列的值来作为结果集总数的近似值。 优化union查询mysql总是通过创建并填充临时表的方式来执行union查询。因此很多优化策略在union查询中都没法很好的使用，经常需要手工的将where、limit、order by等子句下推到union的各个子查询中，以便优化器可以充分利用这些条件尽心优化。除非切实需要服务器消除重复的行，否则就一定要使用union all，如果没有all关键字，mysql会给临时表加上distinct选项，会导致对整个临时表的数据做唯一性检查，代价非常高。 静态查询分析percon toolkit 中的pt-query-advisor能够解析查询日志、分析查询模式、然后给出所有可能存在潜在问题的查询，并给出足够详细的建议，像是给mysql所有的查询做一次全面的健康检查。 参考（1）《高性能MySQL》]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql高性能索引]]></title>
    <url>%2F2018%2Fmysql-index.html</url>
    <content type="text"><![CDATA[索引类型索引由很多类型，可以为不同场景提供更好的性能，在mysql中索引是存储引擎层而不是服务器层实现的。不同存储引擎的索引工作方式也不一样，也不是所有存储引擎支持所有索引类型，其底层实现也可能不同。B-Tree索引使用B-Tree数据结构来存储数据，大多数mysql引擎都支持这种索引，Archive引擎是个例外。存储引擎以不同的方式使用B-Tree索引，性能也各不同，各有优劣。例如，MyISAM使用前缀压缩使得索引更小，但InnoDB按照原数据格式进行存储。再如MyISAM索引通过数据的屋里位置引用被索引的行，而InnoDB根据主键引用被索引的行。B-Tree索引对如下类型的查询有效（假设：key(name,dob)）：1、全值匹配比如查找姓名为Allen的人，出生于1996年的人2、匹配最左前缀比如查找姓名为Allen的人，即只是用索引的第一列3、匹配列前缀比如查找所有以J开头的姓的人。这里也只使用了索引的第一列。4、匹配范围值比如查找姓在Allen和Barrymore之间的人。这里也只使用了索引的第一列。5、匹配某一列并范围匹配另一列比如查找姓名Allen的人，出生1996到2005的人B-Tree支持“值访问索引的查询”，即查询只需要访问索引，而不需要访问数据行，也叫做“覆盖索引”。因为索引树中的节点是有序的，所以除了按值查找之外，索引还可以用order by操作。哈希索引哈希索引基于哈希表实现，只有精确匹配索引所有的列的查询才有效。mysql中只有Memory引擎显式的支持哈希索引，也是Memory的默认索引类型,Memory引擎同时也支持B-Tree索引。Memory引擎是支持费唯一哈希索引的，如果多个列的哈希值相同，索引会以链表的方式存放记录指针到同一个哈希条目中。哈希索引查询的速度非常快，但是哈希索引也有很多限制：1、哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。2、哈希索引数据不是按照索引值顺序存储的，所以也无法用于排序3、哈希索引不支持部分索引列的匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。4、哈希索引只支持等值比较查询,包括=、IN()、&lt;=&gt;，不支持范围查询。5、哈希索引的数据非常快，除非有很多哈希冲突，如果冲突很多的时候索引维护操作的代价也高，查找遍历链表代价也大。因为这些限制，哈希索引只适用于某些特定的场合，而一旦适合则带来的性能提升非常显著。InnoDB引擎有个特殊的功能叫做“自适应哈希索引”。当InnoDB注意到某些索引值被使用的非常频繁时，它会在内存中基于B-tree索引之上再创建一个哈希索引，这样就让B-Tree也有哈希索引的一些优点，比如快速的哈希查找，这是个完全自动的内部行为用户无法控制，但可以关闭该功能。如果存储引擎不支持哈希索引，可以模拟想InnoDB一样创建哈希索引，这样可以享受一些哈希索引的遍历，比如只需要很小的索引就可以为超长的键创建索引，思路很简单：在B-Tree基础上创建一个伪哈希索引。例如：select id from url where url=”http://www.mysql.com“;删除原来url列上的所以，新增一个url_rcc列，使用crc32做哈希，使用下面的方式查询：select id from url where url = “http://www.mysql.com“and url_rcc = crc32(“http://www.mysql.com“);这样做新能会非常高，因为mysql优化器会使用这个选择性很高而体积很小的基于url_crc列的索引来完成查找。这样做需要维护哈希值可以使用触发器或者手动。如果这种方式不要使用sha1()和md5()作为哈希函数，因为这两个函数计算出来的哈希值非常长。空间数据索引（R-Tree）MyISAM表支持空间所以呢，可以用作地理数据存储。全文索引它是查找文本中的关键词，而不是直接比较索引中的值。索引的优点1、大大减少了服务器需要扫描的数据量。2、可以帮助服务器避免排序和临时表。3、可以随机I/O变为顺序I/O。高性能索引策略独立的列“独立的列”是指索引列不能使表达式的一部分也不能是函数的参数例如：select id from aa where id+1=5;正确写法：select id from aa where id=4;前缀索引和索引的选择性有时候需要索引很长的字符列，这会让索引变得大且慢。一个策略是上面提到的通过模拟哈希索引，另一种就是前缀索引。通常可以索引开始的部分字符，这样可以一打打节约索引空间，提高效率。但这样会降低索引的选择性。索引的选择性是指不重复的索引值和数据表的记录总数的比值，范围。索引的选择性越高则查询效率越高，因为可以过滤掉更多的行。唯一索引的选择性是1，性能是最好的。创建前缀索引：alter table aa add key(name(7))mysql前缀索引无法做order by 和group by，也无法使用前缀索引做覆盖扫描。多列索引给多个列创建独立的单例索引，多列索引大部分情况下并不能提高mysql的查询性能，mysql5.0之后更是引入一种叫索引合并的策略，一定程度上可以使用表上的多个单列索引来定位指定的行。索引合并策略是一种优化的结果，但实际上更多的说明了表上的索引建的非常糟糕：1、当出现服务器对多个索引做相交操作（通常有多个and条件），通常以位置需要一个包含所有相关列的多列索引，而不是多个多里的单列索引。2、当服务器需要对多个索引做联合操作时（通常是多个or条件）。选择合适的索引列顺序讲选择性最高的列放到索引最前列。聚簇索引和非聚簇索引聚簇索引并不是一个单独的索引类型，而是一种数据存储方式。B+Tree结构都可以用在MyISAM和InnoDB上。mysql中，不同的存储引擎对索引的实现方式不同。B+Tree示意图聚集索引和非聚集索引原理图聚簇索引聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。聚簇索引要比非聚簇索引查询效率高很多。聚集索引这种主+辅索引的好处是，当发生数据行移动或者页分裂时，辅助索引树不需要更新，因为辅助索引树存储的是主索引的主键关键字，而不是数据具体的物理地址。InnoDB使用的是聚簇索引，将主键组织到一棵B+树中，而行数据就储存在叶子节点上，若使用”where id = 14”这样的条件查找主键，则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据。若对Name列进行条件搜索，则需要两个步骤：第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。非聚簇索引非聚集索引，类似于图书的附录，那个专业术语出现在哪个章节，这些专业术语是有顺序的，但是出现的位置是没有顺序的。每个表只能有一个聚簇索引，因为一个表中的记录只能以一种物理顺序存放。但是，一个表可以有不止一个非聚簇索引。MyISAM的是非聚簇索引，B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。这里的索引都是非聚簇索引。非聚簇索引的两棵B+树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树覆盖索引覆盖索引又可以称为索引覆盖。解释一： 就是select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。解释二： 索引是高效找到行的一个方法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫做覆盖索引。解释三：是非聚集组合索引的一种形式，它包括在查询里的Select、Join和Where子句用到的所有列（即建立索引的字段正好是覆盖查询语句[select子句]与查询条件[Where子句]中所涉及的字段，也即，索引包含了查询正在查找的所有数据）不是所有类型的索引都可以成为覆盖索引，覆盖索引必须要存储索引列的值，而哈希索引、空间索引、全文索引都不存储列的值，所以mysql只能用B-Tree索引做覆盖索引。不同的存储引擎实现覆盖索引的方式也不同，而且也不是所有的存储引擎都支持覆盖索引（memory存储引擎就不支持覆盖索引，不排除未来支持）覆盖索引的好处1、索引条目通常远小于数据行大小，减少数据访问和I/O开销。2、由于InnoDB聚簇索引，覆盖索引对InnoDb表特别有用，InnoDb的二级索引在叶子节点中保存了行的主键值，如果二级主键可以覆盖查询，可以避免对主键索引的二次查询。使用索引描述来排序mysql使用两种方式可以生成有序的结果：通过排序操作或者按索引顺序扫描。如果explain出来的type列值为“index”，则说明mysql使用了索引扫描来做排序。mysql可以使用同一个索引既满足排序有用于查找，如果可能，设计索引时应该尽可能地同时满足这两种任务。只有当索引的列顺序和order by 子句的顺序完全一致，并且所有列的排序方向（要么都是desc，要么都是asc）都一样时，mysql才能使用索引来对结果做排序。如果查询多张关联表，则只有当order by 子句引用的字段全部为第一个表时，才能使用做排序。有一种情况下order by子句可以不满足索引的最左前缀的要求，就是前导列为常量的时候。如果where子句或者join子句中这些列制定了常量就可以弥补索引的不足。表指定索引 key（a,b,c）例子：where a=1 order by b,c;where a=1 order by b;where a&gt;1 order by a,b;一下是错误的例子：where a =1 order by b desc,c asc;where a =1 order by c;where a&gt;1 order by b,c;where a =1 and b in(1,2) order by c;冗余和重复索引mysql允许在相同的列上创建多个索引。mysql需要单独维护重复的索引，这会影响性能。应该避免创建重复索引，发现以后应该立即移除。下面的代码创建了三个重复的索引：create table text(id int not null PRIMARY KEY,UNIQUE(ID),INDEX(ID))ENGIN=InnoDB;冗余索引和重复索引有些不同，如果创建了索引(A,B),在创建索引(A)就是冗余索引，因为这只是前一个索引的前缀索引。如果在创建(B,A),则不是冗余索引，(B)也不是。冗余索引通常只是对B-tree索引来说的。大多数情况不需要冗余索引，但也有出于性能考虑需要冗余索引，因为扩展已有的索引会导致变得太大，从而影响其他使用该索引的查询的性能。表中的索引越多插入速度回越慢，一般来说，增加新索引将会导致insert、update、delete等操作的速度变慢，特别是新增索引达到了内存瓶颈的时候。未使用的索引有些服务器永远用不到的索引。建议考虑删除。怎样检测未使用的索引请google或者百度。索引和锁索引可以让查询锁定更少的行。InnoDB只有在访问行的时候才会对其加锁，而索引能减少InnoDB访问的行数，从而减少锁的数量。MySQL索引失效的几种情况索引不存储null值更准确的说，单列索引不存储null值，复合索引不存储全为null的值。索引不能存储Null，所以对这列采用is null条件时，因为索引上根本没Null值，不能利用到索引，只能全表扫描。为什么索引列不能存Null值？将索引列值进行建树，其中必然涉及到诸多的比较操作。Null值的特殊性就在于参与的运算大多取值为null。这样的话，null值实际上是不能参与进建索引的过程。也就是说，null值不会像其他取值一样出现在索引树的叶子节点上。不适合键值较少的列（重复数据较多的列）如果某个数据列里包含着许多重复的值，就算为它建立了索引也不会有很好的效果。比如说，如果某个数据列里包含了净是些诸如“0/1”或“Y/N”等值，就没有必要为它创建一个索引。前导模糊查询不能利用索引(like ‘%XX’或者like ‘%XX%’)假如有这样一列code的值为’AAA’,’AAB’,’BAA’,’BAB’ ,如果where code like ‘%AB’条件，由于前面是模糊的，所以不能利用索引的顺序，必须一个个去找，看是否满足条件。这样会导致全索引扫描或者全表扫描。如果是这样的条件where code like ‘A % ‘，就可以查找CODE中A开头的CODE的位置，当碰到B开头的数据时，就可以停止查找了，因为后面的数据一定不满足要求。这样就可以利用索引了。in 失效的情况eq_range_index_dive_limit这个参数影响in是否使用索引， 5.6 默认值是10 5.7 默认值是 200mysql自动优化如果mysql估计使用全表扫描要比使用索引快,则不使用索引。如果需要使用索引可以强制使用索引。mysql强制使用索引:force index(索引名或者主键PRI)例如:select from table force index(PRI) limit 2;(强制使用主键)select from table force index(ziduan1_index) limit 2;(强制使用索引”ziduan1_index”)select from table force index(PRI,ziduan1_index) limit 2;(强制使用索引”PRI和ziduan1_index”)mysql禁止某个索引：ignore index(索引名或者主键PRI)例如:select from table ignore index(PRI) limit 2;(禁止使用主键)select from table ignore index(ziduan1_index) limit 2;(禁止使用索引”ziduan1_index”)select from table ignore index(PRI,ziduan1_index) limit 2;(禁止使用索引”PRI,ziduan1_index”)隐式转换索引字段类型使用explain extended select uid from user where mo = 123445 limit 0,1原因：索引隐式转换,mo是varchar类型,这里使用的是数字,索引使用无效（数据类型以及字符集定义不当导致）在JOIN操作中（需要从多个数据表提取数据时），MYSQL只有在主键和外键的数据类型相同时才能使用索引，否则即使建立了索引也不会使用索引列不能参与计算保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);Explain优化查询检测禁止缓存在测试sql语句性能时有时需要禁用缓存， 下面是几种不同的实现方式， 供参考使用：通过sql的select语句中添加SQL_NO_CACHE修饰来禁用查询缓存SELECT SQL_NO_CACHE * FROM TABLE_NAME通过set 变量来实现禁用缓存SET SESSION query_cache_type=0;通过reset指令来重置缓存RESET QUERY CACHE以上三种方法都可以达到测试mysql性能时清缓存的目的。MySQL执行计划调用方式1.EXPLAIN SELECT ……变体：2.EXPLAIN EXTENDED SELECT ……将执行计划”反编译”成SELECT语句，运行SHOW WARNINGS可得到被MySQL优化器优化后的查询语句3.EXPLAIN PARTITIONS SELECT ……用于分区表的EXPLAIN生成QEP的信息关于explain选项下面是一个实例：123456mysql&gt; explain select products_id from products limit 1;+----+-------------+----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | products | index | NULL | PRIMARY | 4 | NULL | 3113 | Using index |+----+-------------+----------+-------+---------------+---------+---------+------+------+-------------+ idMySQL Query Optimizer选定的执行计划中查询的序列号。表示查询中执行select子句或操作表的顺序，id值越大优先级越高，越先被执行。id相同，执行顺序由上至下 select_type1、SIMPLE：简单的select查询，不使用union及子查询2、PRIMARY：最外层的select查询3、UNION：UNION中的第二个或随后的select查询，不依赖于外部查询的结果集4、DEPENDENT UNION：UNION中的第二个或随后的select查询，依赖于外部查询的结果集5、UNION RESULT： UNION查询的结果集SUBQUERY子查询中的第一个select查询，不依赖于外部查询的结果集6、DEPENDENT SUBQUERY:子查询中的第一个select查询，依赖于外部查询的结果集DERIVED用于from子句里有子查询的情况。 MySQL会递归执行这些子查询，把结果放在临时表里。7、UNCACHEABLE SUBQUERY:结果集不能被缓存的子查询，必须重新为外层查询的每一行进行评估8、UNCACHEABLE UNION:UNION中的第二个或随后的select查询，属于不可缓存的子查询 table1、system：表仅有一行(系统表)。这是const连接类型的一个特例。2、const：const用于用常数值比较PRIMARY KEY时。当查询的表仅有一行时，使用system。3、eq_ref：除const类型外最好的可能实现的连接类型。它用在一个索引的所有部分被连接使用并且索引是UNIQUE或PRIMARY KEY， 对于每个索引键，表中只有一条记录与之匹配。4、ref：连接不能基于关键字选择单个行，可能查找到多个符合条件的行。叫做ref是因为索引要跟某个参考值相比较。 这个参考值或者是一个常数，或者是来自一个表里的多表查询的结果值。5、ref_or_null：如同ref，但是MySQL必须在初次查找的结果里找出null条目，然后进行二次查找。6、index_merge：说明索引合并优化被使用了。7、unique_subquery：在某些IN查询中使用此种类型，而不是常规的ref： value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery在某些IN查询中使用此种类型，与unique_subquery类似，但是查询的是非唯一性索引： value IN (SELECT key_column FROM single_table WHERE some_expr)8、range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。 当使用=、&lt;&gt;、&gt;、&gt;=、&lt;、&lt;=、IS NULL、&lt;=&gt;、BETWEEN或者IN操作符，用常量比较关键字列时，可以使用range。9、index：全表扫描，只是扫描表的时候按照索引次序进行而不是行。主要优点就是避免了排序，但是开销仍然非常大。10、all：最坏的情况，从头到尾全表扫描 type表示MySQL在表中找到所需行的方式，又称“访问类型”，常见类型如下: ALL, index, range, ref, eq_ref, const, system, NULL从左到右，性能从最差到最好a. ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行b. index：Full Index Scan，index与ALL区别为index类型只遍历索引树c. range:索引范围扫描，对索引的扫描开始于某一点，返回匹配值域的行。显而易见的索引范围扫描是带有between或者where子句里带有&lt;, &gt;查询。当mysql使用索引去查找一系列值时，例如IN()和OR列表，也会显示range（范围扫描）,当然性能上面是有差异的。d. ref：使用非唯一索引扫描或者唯一索引的前缀扫描，返回匹配某个单独值的记录行e. eq_ref：类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件f. const、system：当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量g. NULL：MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用,如果为空，说明没有可用的索引 key显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULLmysql实际从possible_key选择使用的索引。如果为null，则没有使用索引。很少的情况下，mysql会选择优化不足的索引。这种情况下，可以在select语句中使用use index（indexname）来强制使用一个索引或者用ignore index（indexname）来强制mysql忽略索引 key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的）在不损失精确性的情况下，长度越短越好 ref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 extra1、Distinct一旦mysql找到了与行相联合匹配的行，就不再搜索了。2、Not existsmysql 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。3、Range checked for eachRecord（index map:#）没有找到理想的索引，因此对于从前面表中来的每一个行组合，mysql检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一。4、Using filesort表示MySQL会对结果使用一个外部索引排序，而不是从表里按索引次序读到相关内容。可能在内存或者磁盘上进行排序。MySQL中无法利用索引完成的排序操作称为“文件排序”。5、Using index该值表示相应的select操作中使用了覆盖索引（Covering Index）,列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。6、Using temporarymysql需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上。表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询这个值表示使用了内部临时(基于内存的)表。一个查询可能用到多个临时表。有很多原因都会导致MySQL在执行查询期间创建临时表。两个常见的原因是在来自不同表的上使用了DISTINCT,或者使用了不同的ORDER BY和GROUP BY列。可以强制指定一个临时表使用基于磁盘的MyISAM存储引擎。这样做的原因主要有两个：1)内部临时表占用的空间超过min(tmp_table_size，max_heap_table_size)系统变量的限制2)使用了TEXT/BLOB 列7、Using where使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题。。许多where条件里涉及索引中的列，当（并且如果）它读取索引时，就能被存储引擎检验，因此不是所有带where字句的查询都会显示”Using where”。有时”Using where”的出现就是一个暗示：查询可受益与不同的索引。8、Using join buffer改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。9、Impossible where这个值强调了where语句会导致没有符合条件的行。10、 Select tables optimized away这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行.11、Index merges当MySQL 决定要在一个给定的表上使用超过一个索引的时候，就会出现以下格式中的一个，详细说明使用的索引以及合并的类型。Using sort_union(…)Using union(…)Using intersect(…) 总结：• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况• EXPLAIN不考虑各种Cache• EXPLAIN不能显示MySQL在执行查询时所作的优化工作• 部分统计信息是估算的，并非精确值• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。参考（1）《高性能MySQL》（2）《数据结构与算法分析》]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql使用经典案例]]></title>
    <url>%2F2018%2Fmysql-classic-case.html</url>
    <content type="text"><![CDATA[索引最佳实践无索引案例（添加索引没生效）索引字段类型使用explain extended select uid from user where mo = 123445 limit 0,1原因：索引隐式转换,mo是varchar类型,这里使用的是数字,索引使用无效（数据类型以及字符集定义不当导致）分页优化案例 数据库锁ddl操作需要注意慢查询 延迟 参数优化cpu 100% conn 100% iops 100%disk 100%mem 100% 参考视频：云数据库使用十大经典案例]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剧场版《Fate/stay night [Heaven's Feel] I.presage flower》主题曲]]></title>
    <url>%2F2018%2Ffate.html</url>
    <content type="text"><![CDATA[作曲 : 梶浦由記作词 : 梶浦由記その日々は夢のように那些如梦似幻的日子已然逝去臆病な微笑みと徒然留下的是やさしい爪を怯懦的笑容残して行った还有温柔的指尖退屈な花びらのように宛如毫无生趣的花瓣くるしみを忘れて将痛苦忘却貴方の背中でそっと在你的背后悄然泣いて笑った流着泪笑了帰らぬ日々を思うような似是怀念不再归来的往日一般奇妙な愛しさに満ちた于满盈着奇妙爱怜的箱庭の中で庭园盆景之中息をひそめ敛藏气息季節が行くことを忘れ季节忘记流转静かな水底のような置身于寂静水底般的時間にいた时光之中冷たい花びら冰冷的花瓣夜に散り咲く在夜晚绽放纷飞まるで白い雪のようだね好似白雪一般呢切なく那些揪心地貴方の上に降った飘落你身的かなしみを全て所有悲伤払いのけてあげたいだけ我不过是想为你将其尽数拂去貴方のこと傷つけるもの全て伤害你的一切事物私はきっと許すことは出来ない我绝对无法宽恕優しい日々那些安详的日子涙が出るほど我想回到那时帰りたい到了泪水夺眶而出的程度貴方と二人で与你一同見上げた仰望过的花瓣花びらが散った已凋零飞散月が雲に隠れて月亮隐匿于云层之中貴方は道を失くして令你迷失道路泣き出しそうな你的双眼目をしてた泫然欲泣ぎざぎざなこころだって纵然是你我两人那残破如锯齿的心ふたつ合わせてみれば若能得以结合優しいものがきっと一定能够孕育出生まれてくるわ纯真美好之物啊私を傷つけるものを伤害我的事物貴方は許さないでくれた你未曾给予宽恕それだけでいいの仅是如此我已满足戯れに伸ばされた嬉闹中你向我伸出的手貴方の手にしがみ付いた我将其紧紧握住諦めていた世界に为已然断念的世界やがて温かな灯がともる须臾间点亮温暖的灯火冷たい花びら冰冷的花瓣夜を切り裂く劈裂长夜私が摘んだ光をみんな束ねて我将采撷的所有光芒包捆成束貴方の上に全部只是想要由你之上よろこびのように宛如庆贺地撒き散らしてあげたいだけ将光芒尽数为你挥洒わるいことをしたらきっと貴方が如若我犯下歹行 你一定会对我发怒怒ってくれると約束したよね我们有过这样的约定对吧だからきっともう一度因而你一定能再度私を見つけてくれるよね找寻到我的对吧寂しいところに我已经不必置身于もういなくていいね那孤独寂寥之地了对吧一人で我孤身一人見上げた仰望过的花瓣花びらが散った已凋零飞散]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>动漫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先队列(堆)]]></title>
    <url>%2F2017%2Fpriority-queue.html</url>
    <content type="text"><![CDATA[优先队列(priority queue)普通的队列是一种先进先出的数据结构，元素在队列尾追加，而从队列头删除。在优先队列中，元素被赋予优先级。当访问元素时，具有最高优先级的元素最先删除。优先队列具有最高级先出 （first in, largest out）的行为特征。模型优先队列允许至少两种操作的数据结构：insert和deleteMin（删除最小者），它是找出、返回并删除优先队列中最小的元素。insert等价于入队，deleteMin等价出队。二叉堆结构性质二叉堆是一种特殊的堆，二叉堆是完全填满的二元树（二叉树）或者是近似完全填满的二元树（二叉树）。二叉堆有两种：最大堆和最小堆。最大堆：父结点的键值总是大于或等于任何一个子节点的键值；最小堆：父结点的键值总是小于或等于任何一个子节点的键值。因为完全二叉树很有规律，所以它可以用一个数组标识而不需要使用链。后面始终把堆画成树，具体实现将使用简单的数组一个堆结构将由一个（comparable对象的）数组和一个代表当前堆大小的整数组成。堆序性质让操作快速执行的性质是堆序性质(heap-order property)。由于想要快速找出最小元素，因此最小元素应该在根上。因此以常数时间得到附加操作findMin。基本的堆操作所有的工作都需要保证始终保持堆序性质。insert操作为将一个元素X插入到堆中，在下一个可用位置创建一个空穴，如果X可以放在该空穴中而并不破坏堆的序，那么插入完成。否则，把空穴的父节点上的元素移入该空穴中，这样，空穴就朝着跟的方向向上冒一步，继续该过程直到X能被放入空穴为止。deleteMin删除根元素，根节点建立一个空穴，将空穴的两个儿子中较小者移入空穴，这样就把空穴乡下推了一层，重复该步骤直到X可以被放入空穴中。需要考虑堆中存在偶数个元素的时候，将遇到一个节点只有一个儿子的情况优先队列的应用选择问题java标准库中的优先队列未完待续]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树]]></title>
    <url>%2F2017%2Ftree.html</url>
    <content type="text"><![CDATA[二叉树二叉树(binary tree)是一棵树，其中每个节点都不能有多余两个的子节点。二叉树的一个性质是一颗平均二叉树的深度要比节点个数N小得多，这个性质有时候很重要。实现因为一个二叉树节点最多有两个子节点，所以可以保存直接链接到它们的链。树节点的声明在结构上类似于双链表的声明，在声明中，节点就是有element的信息加上两个到其他节点的引用(left和right)组成的结构123456class BinaryNode&#123; Object element; BinaryNode left; BinaryNode right;&#125; 例子：表达式树表达式树的树叶是操作数，比如：常数或者变量名，而其他的节点为操作符。由于所有的操作都是二元的，因此这棵树正好是二叉树。如下图： 查找树ADT——二叉查找树二叉树的一个重要应用就是它们在查找中的使用。使二叉树成为查找树的性质是，对于树中的每个节点X，它的左子树中所有想的值小于X中的项，而它右子树中所有项的值大于X中的项。如下图(假设节点元素都是整数)：二叉树查找树要求所有的项都能够排序，需要写出一个interface来标识这个性质，这个接口就是Comparable。该接口告诉我们树种的两项总可以使用compareTo方法进行比较。由此可以确定所有其他可能的关系，特别是不适用equals方法，而是根据两项相等当且仅当compareTo方法返回0来判断相等。 contains方法如果在树T中存在还有项X的节点，那么这个操作需要返回true，如果这样的节点不存在则返回false。 findMin方法和findMax方法这两个private分别返回树中包含最小和最大元素的节点的引用。执行findMin从根开始并且只要有左节点就向左进行，终点就是最小的元素，findMax向右同理。 insert方法为了将X插入到树T中，可以像用contains那样沿着树查找。如果找到X则什么也不做(或者做一些“更新”)，否则将X插入到遍历的路劲上的最后一点上。重复元素的插入可以通过在节点记录中保留一个附加域以指示发生的频率来处理。这对整个树增加了某些附加空间，但是却比将重复信息放到树中要好（它将使树的深度变得很大） remove方法如果节点是一片树叶则立即删除。如果节点有一个子节点，则该节点可以在其父节点调整自己的链以绕过该节点后被删除。如果该节点有两个子节点，一般的删除策略是用其右子树的最小数据代替该节点的数据并递归地删除那个节点(现在它是空的)，因为右子树中的最小的节点不可能有左节点，所以第二次remove要容易。如果删除的次数不多，通常使用的策略是懒惰删除，当一个元素要被删除时，它仍被保留在树中，而是被标记为删除，这在有重复项时很常用，因为此时记录出现频率数的域可以减1. AVL树AVL树是带有平衡条件的二叉查找树。这个平衡条件必须要容易保持，而且它保证树的深度需是O(log N)。最简单的想法是要求左右子树具有相同的高度。另一种平衡条件是要求每个节点都必须有相同高度的左子树和右子树。 单旋转 双旋转 树的遍历遍历的一般方法是首先处理左子树，然后是当前节点，最后是右子树。这个算法的有趣部分除它简单的特性外，还在于其总的运行时间是O(N)。 标准库中的集合与映射List容器即ArrayList和Linkedlist用于查找效率很低。因此,Collections API提供了两个附加容器Set和Map，它们对诸如插入、删除、和查找等基本操作提供有效的实现。 关于Set接口Set接口代表不允许重复元素的Collection。由接口SortedSet给出的一种特殊类型的Set保证其中的各项处于有序的状态。 关于Map接口Map是一个接口，代表由关键字以及它们的值组成的一些项的集合。关键字必须是唯一的，但是若干关键字可以映射到一些相同的值。在SortMap接口中，映射中的关键字保持逻辑上有序的状态。通过一个Map进行迭代要比Colection复杂，因为Map不提供迭代器而是提供3种方法讲Map对象的视图最为Collection对象返回。由于这些视图本身就是Collection，因此它们可以被迭代。如下：123Set&lt;KeyType&gt; keySet()Collection&lt;ValueType&gt; values()Set&lt;Map.Entry&lt;keyType.ValueType&gt;&gt; entrySet() TreeSet类和TreeMap类的实现Java要求TreeSet和TreeMap支持基本的add、remove和contains操作以对数最坏情形时间完成，因此基本的实现方法就是平衡二叉查找树。一般并不适用AVL树，而是使用一些自顶向下的红黑树。 小结表达式树是更一般结构即所谓分析树的一个小例子，分析树是编译器设计中的核心数据结构。分析树不是二叉树，而是表达式树相对简单的扩充。查找树在算法实际中是非常重要的，几乎支持所有有用的操作，而其对数平均开销很小。查找树的问题在于其性能严重依赖输入，而输入是随机的。处理这个问题的几种平衡树方案：AVL数、伸展树、B树等。在实践中，所有平衡树方案的运行时间对于插入和删除操作(除查找稍微快一些)都不如简单二叉树省时，但一般来说是可以接受的，它防止轻易得到最坏情形的输入。通过将一些元素插入到查找树然后执行一次中序遍历，我们得到的是拍过顺序的元素。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[散列]]></title>
    <url>%2F2017%2Fhash.html</url>
    <content type="text"><![CDATA[散列散列表的实现常常叫做散列(hashing)。散列是一种用于以常数平均时间执行插入、删除和查找的技术。但是，那些需要元素间任何排序信息的树操作将不会得到有效的支持。散列表散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。哈希函数给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。所有散列函数都有如下一个基本特性：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果。但另一方面，散列函数的输入和输出不是一一对应的，如果两个散列值相同，两个输入值很可能是相同的，但不绝对肯定二者一定相等（可能出现哈希碰撞）。输入一些数据计算出散列值，然后部分改变输入值，一个具有强混淆特性的散列函数会产生一个完全不同的散列值。散列的价值散列的价值在于速度假如键没有按照一定的顺序进行保存，那么查询的时候就只能按照顺序进行线性查询，然而，线性查询是最慢的查询方式。所以，将键值按照一定的顺序排序，并且使用二分查找能购有效的提升速度。散列在此之上，更近一步，他将键保存在数组中(数组的查询速度最快)，用数组来表示键的信息，但是由于Map的容量是可变的，而数组的容量是不变的。要解决这个问题，数组中存的并不是键本身，而是键对象生成的一个数字，将其作为数组的下标，这个数字就是散列码。而这种办法所产生的问题就是下标重复。而我们的解决办法就是配合equals来确定键值。查询的过程首先就是计算散列码，然后用散列码来查询函数(下标)，通常，我们的数组中保存的是值的list，因此，我们计算出散列码之后，通过下表取到的对应部分的list，然后通过equals就可以快速找到键值。处理冲突方法分离链接法其做法是将散列到同一个值的所有元素保存到一个表中。执行一次查找，使用散列函数来确定究竟遍历哪个链表。然后在被确定的链表中执行一次查找。执行一次插入，检查响应的链表看看该元素是否已经处在适当的位置(如果允许插入重复元素，那么要留出一个额外的域，这个域当出现匹配事件时增1)。如果这个元素是新的元素，那么它将被插入到链表前端，这个不仅因为方便，还因为常常发生这样的事实：新近插入的元素最有可能不久又被访问。除链表外，任何方案都可以解决冲突现象，一颗二叉树或者另一个散列表都可以，但是最好的解决方案是散列函数是最好的，那么所有的链表都应该是短的。开放定址法分离链接散列算法的缺点是使用一些链表。由于给新单元分配地址需要时间，因此倒着算法的速度有些减慢，同时算法实际上还要求对第二种数据结构的实现。另一种不用链表解决冲突的方法是尝试另外一些单元，知道找出空的单元为止。因为所有大数据都要放入表内，所以这种解决方案所需要的表要比分离链接散列的表大。一般来说对于不适用分离链接的散列表来说其装填因子应该低于0.5，这样的表叫做侦探散列表。这种通常的冲突解决方案有三种。线性侦探法平均探测法双散列法再散列法对于使平方探测的开放定址散列法，如果散列表填的太满，那么操作的运行时间将开始消耗过长，且插入操作可能失败。此时，一种解决方案是建立另外一个大约两倍的大的表，而且使用一个相关的新散列函数，扫描整个原始散列表，计算每个元素的新散列值并将其插入到新表中，整个操作就是叫做再散列。java标准库中的散列表标准库包括Set和Map的散列表的实现：HashSet和HashMap类。HashSet中的项必须提供equals和hashCode方法。它们通常是用分离链接散列实现的。散列表操作中费时多的部分就是计算hashCode方法，String类中的hashCode有个重要优化：每个String对象内部都存储它的hashCode值，初始为0，若hashCode方法被调用，那么就记住这个值，ru过hashCode对同一个String对象第二次计算，则可以避免昂贵的重新计算，这个技巧叫做闪存散列代码。闪存散列代码之所以有效，是因为String类是不可改变的：要是String允许变化，那么它就会使hashCode无效，而重置回0.可扩散列处理数据量太大以至于装不进主存的情况。查找性能分析散列表的查找过程基本上和造表过程相同。一些关键码可通过散列函数转换的地址直接找到，另一些关键码在散列函数得到的地址上产生了冲突，需要按处理冲突的方法进行查找。在介绍的三种处理冲突的方法中，产生冲突后的查找仍然是给定值与关键码进行比较的过程。所以，对散列表查找效率的量度，依然用平均查找长度来衡量。查找过程中，关键码的比较次数，取决于产生冲突的多少，产生的冲突少，查找效率就高，产生的冲突多，查找效率就低。因此，影响产生冲突多少的因素，也就是影响查找效率的因素。影响产生冲突多少有以下三个因素：1．散列函数是否均匀；处理冲突的方法；3．散列表的装填因子。散列表的装填因子定义为：α= 填入表中的元素个数/散列表的长度α是散列表装满程度的标志因子。由于表长是定值，α与“填入表中的元素个数”成正比，所以，α越大，填入表中的元素较多，产生冲突的可能性就越大；α越小，填入表中的元素较少，产生冲突的可能性就越小。实际上，散列表的平均查找长度是装填因子α的函数，只是不同处理冲突的方法有不同的函数。常用hash算法：（1)MD4（2)MD5（3)SHA-1及其他]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[表、栈和队列]]></title>
    <url>%2F2017%2Flist.html</url>
    <content type="text"><![CDATA[ADT抽象数据类型(abstract data type,ADT)是带有一组操作的对象的集合。对于集合ADT，可以有添加、删除、包含等操作表ADT简单数组实现对表的所有操作都可以通过使用数组来实现。虽然数组是有固定容量创建的，但在需要的时候可以使用双倍的容量创建一个不同的数组。许多情形下表是通过在末端进行插入操作的，然后只对数组访问。这种情形下数组是一种恰当的实现。然而如果发生一些插入和删除操作，特别是前端进行，那么数组不是一种好的选择。简单链表为了避免插入和删除的线性开销，需要保证表可以不连续存储，否则表的每个部分都可能需要整体移动。链表是由一系列的节点组成，这些节点不必在内存中相连，每个节点含有表元素和到包含该元素后继元素的节点的链，可以称之为next链，最后一个单元的next链引用null。简单链表删除最后一项比较复杂，因为必须找出指向最后节点的项，把它的next链改成null，然后在更新持有最后节点的链，最好的做法是让每一个节点有一个指向它在表中的前面节点的链称之为双链表java Collection API中的表Collection接口Collection接口扩展了Iterable接口，实现Iterable接口的类拥有增强for循环，都可以使用forEach进行循环遍历123456789public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; &#123; int size(); boolean isEmpty(); void clear(); boolean contains(Object o); Iterator&lt;E&gt; iterator(); boolean add(E e); boolean remove(Object o); &#125; Iterator接口1234567public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); default void remove() &#123; throw new UnsupportedOperationException(&quot;remove&quot;); &#125;&#125; Iterator的remove方法主要优点在于：Collection的remove方法必须先找出需要删除的项。在迭代集合时Collection的remove会抛出ConcurrentModificationException 增强for循环java中的增强for循环实际上编译器会重写成如下所示：1234567891011List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("abc");for (String s : list) &#123; System.out.println(s);&#125;//等同于上面增强for循环写法Iterator&lt;String&gt; it = list.iterator();while (it.hasNext()) &#123; String s = it.next(); System.out.println(s);&#125; List接口、ArrayList类和LinkedList类1、ArrayList类提供了list ADT的一种可增长数组的实现，其优点在于对get和set的调用花费常数时间，其缺点是插入和删除代价昂贵（除了在末端进行）。2、LinkedList类提供了list ADT的双链表实现，其优点是插入和删除均开销很小，在表的前端和末端添加和删除都是常数时间的操作，其缺点是不容易索引，get的调用是昂贵的（除了get第一个和最后一个）。3、对搜索而言，ArrayList和LinkedList都是低效的，对Collection的contains和remove方法的调用均花费线性时间。4、ArrayList中有个容量的概念，它标识基础数组的大小，在需要的时候会自动扩容保证至少具有表的大小，如果早期知道该大小，可以设置容量足够大的量以避免数组容量以后的扩展，trimToSize可以在所有的ArrayList添加操作完成之后使用以避免浪费空间。5、以下方法对于LinkedList操作整个程序线性时间不是二次时间，对于ArrayList是二次时间，因为对于ArrayList即使迭代器位于需要被删除的节点上，其remove方法仍然是昂贵的，因为数组的项必须要移动12345678public static void remove(List&lt;Integer&gt; list) &#123; Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; if (iterator.next() % 2 == 0) &#123; iterator.remove(); &#125; &#125; &#125; ListIterator接口ListIterator扩展了Iterator接口。1、iterator可以应用于所有的集合，Set、List和Map以及这些集合的子类型。而ListIterator只能用于List及其子类型。2、ListIterator有hasPrevious()和previous()方法，可以实现逆向遍历，但是iterator不可以。3、ListIterator可以定位当前索引的位置，nextIndex()和previousIndex()可以实现。Iterator没有此功能。4、ListIterator有add方法，可以向List中添加对象，而Iterator不能。5、ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能实现修改。都可以实现删除操作。用例：它可以用来从List的所有的偶数中减去1，对于LinkedList来说，不适用ListIterator的set方法是很难做到的。 简单的ArrayList类的实现只供参考理解，编译器会报错123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class MyArrayList&lt;AnyType&gt; implements Iterable&lt;AnyType&gt; &#123; private static final int DEFAULT_CAPACITY = 10; private int theSize; private AnyType[] theItems; public int size() &#123; return theSize; &#125; public boolean isEmpty() &#123; return size() == 0; &#125; //调整容量符合大小 public void trimToSize() &#123; ensureCapacity(size()); &#125; //确保数组大小足够大 public void ensureCapacity(int newCapacity) &#123; if(newCapacity &lt; theSize) return; //复制数据到新数组中 AnyType[] old = theItems; theItems = (AnyType[]) new Object[newCapacity]; for(int i = 0; i &lt;size(); i++) &#123; theItems[i] = old[i]; &#125; &#125; public AnyType get(int index) &#123; if(index &lt; 0 || index &gt;= size()) &#123; throw new ArrayIndexOutOfBoundsException(); &#125; return theItems[index]; &#125; public AnyType set(int index, AnyType newVal) &#123; if(index &lt; 0 || index &gt;= size()) &#123; throw new ArrayIndexOutOfBoundsException(); &#125; AnyType old = theItems[index]; theItems[index] = newVal; return old; &#125; public void add(int index, AnyType x) &#123; //数组不够大，则扩大数组 if(theItems.length == size()) &#123; ensureCapacity(size()*2 + 1); &#125; //从index开始，元素往后移动一位 for(int i = theSize; i &gt; index; i--) &#123; theItems[i] = theItems[i - 1]; &#125; //index位置赋值x theItems[index] = x; theSize++; &#125; public AanyType remove(int index) &#123; AnyType removedItem = theItems[index]; for(int i = index; i &lt; size(); i++) &#123; //从index位置开始，所有元素都往前移动一位 theItems[i] = theItems[i + 1]; &#125; theSize--; return removedItem; &#125; public java.util.Iterator&lt;AnyType&gt; iterator() &#123; return new ArrayListIterator&lt;AnyType&gt;(); &#125; private static class ArrayListIterator&lt;AnyType&gt; implements java.util.Iterator&lt;AnyType&gt; &#123; private int current = 0; public boolean hasNext() &#123; return current &lt; MyArrayList.this.size(); &#125; public AnyType next() &#123; return MyArrayList.this.theItems[current++]; &#125; public void remove() &#123; //防止迭代器的remove与MyArrayList的remove冲突 MyArrayList.this.remove(--current); &#125; &#125; &#125;; 简单的LinkedList类的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176public class MyLinkedList &lt;AnyType&gt; implements Iterable&lt;AnyType&gt;&#123; private int theSize; //双向链表中的元素个数 private int modCount; //这个标记为了配合Iterator实现修改的保护，这一点后面专做论述,凡是做了增删修改，这个标记均变化 private Node&lt;AnyType&gt; beginMarker; // 双向链表的开始标记 private Node&lt;AnyType&gt; endMarker; //双向链表的尾部标记 public MyLinkedList() &#123; // 构造函数 先初始化双向聊表 调动 clear()函数 clear(); &#125; public void clear() &#123;// 确保双向链表处于空的状态 ----&gt; 我们使用一个辅助的头结点 // 头标记和尾标记 指向同一个 辅助头结点,和一个辅助的尾节点 beginMarker = new Node&lt;AnyType&gt;(null, null, null); endMarker = new Node&lt;AnyType&gt;(null, beginMarker, null); beginMarker.next = endMarker; theSize = 0; modCount ++; //zhege &#125; // 获取元素的个数 public int size() &#123; return theSize; &#125; // 判断是否为空 public boolean isEmpty() &#123; return theSize == 0; &#125; /* * 增删查改的操作 */ // 默认把元素插入到尾部,其中调用插入到指定位置的函数 public boolean add(AnyType x) &#123; add(size()+1, x); return true; &#125; // 把元素插入到指定位置，其中调用插入到指定元素之前 函数 public void add(int idx, AnyType x) &#123; addBefore(getNode(idx), x); &#125; // 重置某个节点的data值，并返回以前的 data值 public AnyType set(int idx, AnyType newVal) &#123; if(idx &lt;1 || idx &gt;size()) throw new RuntimeException(new Exception("下表越界")); Node&lt;AnyType&gt; p = getNode(idx); AnyType oldVal = p.data; p.data = newVal; return oldVal; &#125; // 删除第idx个节点,调用remove(Node)函数，返回删除节点的data值 public AnyType remove(int idx) &#123; if(idx &lt;1 || idx &gt;size()) throw new RuntimeException(new Exception("下表越界")); return remove(getNode(idx)); &#125; /* * 下面这些函数都是一些private的都是位别的一些函数服务的 */ // 在p前面插入 x 元素 private void addBefore(Node&lt;AnyType&gt;p, AnyType x) &#123; Node&lt;AnyType&gt; newNode = new Node&lt;AnyType&gt;(x, p.prev, p); newNode.prev.next = newNode; p.prev = newNode; theSize ++; //添加进来一个新元素之后，别忘了元素个数++ modCount ++; //无论增删 该标志 均++ &#125; // 获取 idx处的 节点引用 private Node&lt;AnyType&gt; getNode(int idx) &#123; if(idx &lt; 1 || idx &gt; size()+1)// 考虑在尾部插入的情况，如果取这个尾节点，其data = null throw new RuntimeException(new Exception("索引越界")); Node&lt;AnyType&gt; p = null; if( idx &lt;= size()/2) // 在前半边中找 &#123; p = beginMarker.next; for( int i = 1; i &lt; idx; i++) p = p.next; &#125;else&#123; //在后半边中找 p = endMarker; for(int i = size(); i &gt;= idx; i--) p = p.prev; &#125; return p; &#125; // 返回 删除某个节点，并返回这个节点的data值 private AnyType remove(Node&lt;AnyType&gt; p) &#123; p.prev.next = p.next; p.next.prev = p.prev; theSize --; modCount --; return p.data; &#125; /* * 实现迭代器 */ public Iterator&lt;AnyType&gt; iterator() &#123; return new LinkedListIterator(); &#125; //实现迭代器 private class LinkedListIterator implements Iterator&lt;AnyType&gt; &#123; private Node&lt;AnyType&gt; current = beginMarker.next; //记住当前的位置，这和书序表中类似 private int expectedModCount = modCount; private boolean okToRemove = false; @Override public boolean hasNext() &#123; // TODO Auto-generated method stub return current!=endMarker; &#125; @Override public AnyType next() &#123; // 注意了 下面的 保护迭代期间 不允许 越过迭代器修改集合元素的 机制 是精髓 if(modCount != expectedModCount) throw new RuntimeException(new Exception("您刚刚越过迭代器修改了集合元素")); if(!hasNext()) throw new RuntimeException(new Exception("已经没有元素了")); AnyType nextItem = current.data; current = current.next; okToRemove = true; return nextItem; &#125; @Override public void remove() &#123; // TODO Auto-generated method stub if(modCount != expectedModCount) throw new RuntimeException(new Exception("您刚刚越过迭代器修改了集合元素")); if(!okToRemove) throw new RuntimeException(new Exception("先next再删除")); MyLinkedList.this.remove(current.prev); okToRemove = false; // 与next()中的 okToRemove = false； 遥相呼应，以确保必须在next()之后才能remove expectedModCount ++; &#125; &#125; /* * 私有嵌套类的形式，定义内部节点，节点里面没有访问双向链表中的内容，所以使用私有嵌套类可也 * 如果访问了外面类的属性或者方法就只能使用内部类，去除static关键字,内部类的使用主要是为了可以简写，见单链表中的介绍 */ private static class Node&lt;AnyType&gt;&#123; // 构造函数 public Node(AnyType d, Node&lt;AnyType&gt;p, Node&lt;AnyType&gt;n) &#123; data = d; prev = p; next = n; &#125; public AnyType data; public Node&lt;AnyType&gt; prev; public Node&lt;AnyType&gt; next; &#125; &#125; 栈栈(stack)是限制插入和删除只能在一个位置上进行的表，该位置是表的末端，叫做栈的顶端(top),对栈的操作有push(进栈)和pop(出栈)，前者相对于插入，后者相对于删除最后插入的元素。栈有时又叫做LIFO(后进先出)表。 栈的实现由于栈是一个表，任何实现表的方法都能实现栈，ArrayList和LinkedList都支持栈操作 栈的应用简单例子：平衡符号：编译器检查程序的语法错误叙述如下：做一个空栈，读入字符知道文件结尾，如果字符是个开放符号则将其推入栈中，如果是个封闭符号则当栈空时报错，否则将栈元素弹出，如果弹出的符号不是对应的开放符号则报错，在文件结尾如果栈非空则报错。 队列队列也是表，使用队列时，插入在一段，删除则在另一端。队列的基本操作是enqueue(入队)，它在表的末端插入元素，和dequeue(出队)，它删除并返回在表的开头的元素 队列的实现如果栈的情形一样，对于队列而言任何的表的实现都是合法的 队列的应用窗口买票的应用等所有需要先进先出的案例]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigDecimal用法详解]]></title>
    <url>%2F2017%2Fbigdecimal.html</url>
    <content type="text"><![CDATA[BigDecimal的构造方法1234BigDecimal(int) 创建一个具有参数所指定整数值的对象。BigDecimal(double) 创建一个具有参数所指定双精度值的对象。BigDecimal(long) 创建一个具有参数所指定长整数值的对象。BigDecimal(String) 创建一个具有参数所指定以字符串表示的数值的对象。 注意：123BigDecimal a =new BigDecimal(1.22);System.out.println(a);会输出：1.2199999999999999733546474089962430298328399658203125 通常建议优先使用String构造方法。 加减乘除add(BigDecimal) BigDecimal对象中的值相加，然后返回这个对象。subtract(BigDecimal) BigDecimal对象中的值相减，然后返回这个对象。multiply(BigDecimal) BigDecimal对象中的值相乘，然后返回这个对象。divide(BigDecimal) BigDecimal对象中的值相除，然后返回这个对象。BigDecimal都是不可变的（immutable）的， 在进行每一次四则运算时，都会产生一个新的对象 ，所以在做加减乘除运算时要记得要保存操作后的值。 保留小数decimal.setScale(4,BigDecimal.ROUND_HALF_DOWN)第一位是保留多少位小数第二位参数定义 ROUND_CEILINGRounding mode to round towards positive infinity.向正无穷方向舍入 ROUND_DOWNRounding mode to round towards zero.向零方向舍入 ROUND_FLOORRounding mode to round towards negative infinity.向负无穷方向舍入 ROUND_HALF_DOWNRounding mode to round towards “nearest neighbor” unless both neighbors are equidistant, in which case round down.向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向下舍入, 例如1.55 保留一位小数结果为1.5 ROUND_HALF_EVENRounding mode to round towards the “nearest neighbor” unless both neighbors are equidistant, in which case, round towards the even neighbor.向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，如果保留位数是奇数，使用ROUND_HALF_UP ，如果是偶数，使用ROUND_HALF_DOWN ROUND_HALF_UPRounding mode to round towards “nearest neighbor” unless both neighbors are equidistant, in which case round up.向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向上舍入, 1.55保留一位小数结果为1.6 ROUND_UNNECESSARYRounding mode to assert that the requested operation has an exact result, hence no rounding is necessary.计算结果是精确的，不需要舍入模式 ROUND_UPRounding mode to round away from zero.向远离0的方向舍入 BigDecimal进行比较大小可以通过BigDecimal的compareTo方法来进行比较。 例如：new BigDecimal(“1”).compareTo(new BigDecimal(“2”));返回的结果是int类型，-1表示小于，0是等于，1是大于。即左边比右边数大，返回1，相等返回0，比右边小返回-1。注意 不可用equals进行相等的判断，equals 比较是两个BigDecimal对象的地址。 最大、最小值、绝对值、相反数a.max (b) //比较取最大值a.min(b) //比较取最小值a.abs()//取最绝对值a.negate()//取相反数 格式化1234567891011121314public static void main(String[] args) &#123; NumberFormat currency = NumberFormat.getCurrencyInstance(); //建立货币格式化引用 NumberFormat percent = NumberFormat.getPercentInstance(); //建立百分比格式化引用 percent.setMaximumFractionDigits(3); //百分比小数点最多3位 BigDecimal loanAmount = new BigDecimal("15000.48"); //贷款金额 BigDecimal interestRate = new BigDecimal("0.008"); //利率 BigDecimal interest = loanAmount.multiply(interestRate); //相乘 System.out.println("贷款金额:\t" + currency.format(loanAmount)); System.out.println("利率:\t" + percent.format(interestRate)); System.out.println("利息:\t" + currency.format(interest)); &#125;输出：贷款金额: ￥15,000.48 利率: 0.8% 利息: ￥120.00 BigDecimal的3个toString方法BigDecimal类有3个toString方法，分别是toEngineeringString、toPlainString和toStringtoEngineeringString：有必要时使用工程计数法。工程记数法是一种工程计算中经常使用的记录数字的方法，与科学技术法类似，但要求10的幂必须是3的倍数toPlainString：不使用任何指数toString：有必要时使用科学计数法spring mvc 接口返回json里面有BigDecimal对象的时候注意返回的是否是自己想要的。12345678910111213141516import java.math.BigDecimal;public class BigDecimalDemo &#123; public static void main(String[] args) &#123; BigDecimal bg = new BigDecimal("1E11"); System.out.println(bg.toEngineeringString()); System.out.println(bg.toPlainString()); System.out.println(bg.toString()); &#125;&#125; 输出100E+91000000000001E+11 去掉多余的0（stripTrailingZeros）BigDecimal原生提供了stripTrailingZeros方法可以实现去掉末尾的0，然后使用toPlainString可以输出数值，注意这里如果使用toString() 会变成科学计数法输出 其他常用方法int scale() //返回此 BigDecimal 的标度int precision() //返回此 BigDecimal 的精度int signum() //返回此 BigDecimal 的正负号函数BigDecimal ulp() //返回此 BigDecimal 的 ulp（最后一位的单位）的大小]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
</search>
